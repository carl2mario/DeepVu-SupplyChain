{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model for price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used : encoder decoder made of GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tensorflow 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "print('Imported tensorflow', tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folder scripts\n",
    "from feature_selection_feed import top_correlated, reformat_as_sequence\n",
    "from evaluation import score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>f000_open</th>\n",
       "      <th>f000_high</th>\n",
       "      <th>f000_low</th>\n",
       "      <th>f000_settle</th>\n",
       "      <th>f001_open</th>\n",
       "      <th>f001_high</th>\n",
       "      <th>...</th>\n",
       "      <th>f136_open</th>\n",
       "      <th>f136_high</th>\n",
       "      <th>f136_low</th>\n",
       "      <th>f136_settle</th>\n",
       "      <th>f137_open</th>\n",
       "      <th>f137_high</th>\n",
       "      <th>f137_low</th>\n",
       "      <th>f137_settle</th>\n",
       "      <th>week</th>\n",
       "      <th>week_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20081201</td>\n",
       "      <td>444.511058</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>53.08</td>\n",
       "      <td>56.33</td>\n",
       "      <td>52.62</td>\n",
       "      <td>56.29</td>\n",
       "      <td>49.11</td>\n",
       "      <td>52.15</td>\n",
       "      <td>...</td>\n",
       "      <td>9420.0</td>\n",
       "      <td>9680.0</td>\n",
       "      <td>9315.0</td>\n",
       "      <td>9540.0</td>\n",
       "      <td>9520.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9650.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20081202</td>\n",
       "      <td>446.908899</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>459.323035</td>\n",
       "      <td>55.99</td>\n",
       "      <td>56.29</td>\n",
       "      <td>54.68</td>\n",
       "      <td>55.30</td>\n",
       "      <td>51.80</td>\n",
       "      <td>52.24</td>\n",
       "      <td>...</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9430.0</td>\n",
       "      <td>9510.0</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>9730.0</td>\n",
       "      <td>9560.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>20081203</td>\n",
       "      <td>453.484820</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>459.696940</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.72</td>\n",
       "      <td>54.65</td>\n",
       "      <td>55.21</td>\n",
       "      <td>53.01</td>\n",
       "      <td>53.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9580.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9530.0</td>\n",
       "      <td>9690.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>9590.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>20081204</td>\n",
       "      <td>447.532919</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>459.964389</td>\n",
       "      <td>55.50</td>\n",
       "      <td>57.81</td>\n",
       "      <td>54.88</td>\n",
       "      <td>57.62</td>\n",
       "      <td>51.75</td>\n",
       "      <td>54.42</td>\n",
       "      <td>...</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>9145.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9445.0</td>\n",
       "      <td>9205.0</td>\n",
       "      <td>9225.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>20081205</td>\n",
       "      <td>447.084228</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>459.503235</td>\n",
       "      <td>58.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>56.63</td>\n",
       "      <td>56.75</td>\n",
       "      <td>54.70</td>\n",
       "      <td>56.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8710.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>8595.0</td>\n",
       "      <td>8665.0</td>\n",
       "      <td>8885.0</td>\n",
       "      <td>8940.0</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          p0          p1          p2  f000_open  f000_high  \\\n",
       "109  20081201  444.511058  457.032497  457.032497      53.08      56.33   \n",
       "110  20081202  446.908899  465.530103  459.323035      55.99      56.29   \n",
       "111  20081203  453.484820  482.060575  459.696940      56.50      56.72   \n",
       "112  20081204  447.532919  472.395859  459.964389      55.50      57.81   \n",
       "113  20081205  447.084228  471.922241  459.503235      58.50      60.00   \n",
       "\n",
       "     f000_low  f000_settle  f001_open  f001_high    ...      f136_open  \\\n",
       "109     52.62        56.29      49.11      52.15    ...         9420.0   \n",
       "110     54.68        55.30      51.80      52.24    ...         9480.0   \n",
       "111     54.65        55.21      53.01      53.02    ...         9495.0   \n",
       "112     54.88        57.62      51.75      54.42    ...         9485.0   \n",
       "113     56.63        56.75      54.70      56.18    ...         8710.0   \n",
       "\n",
       "     f136_high  f136_low  f136_settle  f137_open  f137_high  f137_low  \\\n",
       "109     9680.0    9315.0       9540.0     9520.0     9800.0    9495.0   \n",
       "110     9600.0    9430.0       9510.0     9640.0     9730.0    9560.0   \n",
       "111     9580.0    9400.0       9500.0     9530.0     9690.0    9505.0   \n",
       "112     9485.0    9120.0       9145.0     9400.0     9445.0    9205.0   \n",
       "113     9000.0    8595.0       8665.0     8885.0     8940.0    8670.0   \n",
       "\n",
       "     f137_settle  week  week_date  \n",
       "109       9650.0  2030   20081201  \n",
       "110       9630.0  2030   20081201  \n",
       "111       9590.0  2030   20081201  \n",
       "112       9225.0  2030   20081201  \n",
       "113       8715.0  2030   20081201  \n",
       "\n",
       "[5 rows x 558 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Bis. Boosted data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_t</th>\n",
       "      <th>p1_(t-99)</th>\n",
       "      <th>p1_(t-98)</th>\n",
       "      <th>p1_(t-97)</th>\n",
       "      <th>p1_(t-96)</th>\n",
       "      <th>p1_(t-95)</th>\n",
       "      <th>p1_(t-94)</th>\n",
       "      <th>p1_(t-93)</th>\n",
       "      <th>p1_(t-92)</th>\n",
       "      <th>p1_(t-91)</th>\n",
       "      <th>...</th>\n",
       "      <th>p1_(t+41)</th>\n",
       "      <th>p1_(t+42)</th>\n",
       "      <th>p1_(t+43)</th>\n",
       "      <th>p1_(t+44)</th>\n",
       "      <th>p1_(t+45)</th>\n",
       "      <th>p1_(t+46)</th>\n",
       "      <th>p1_(t+47)</th>\n",
       "      <th>p1_(t+48)</th>\n",
       "      <th>p1_(t+49)</th>\n",
       "      <th>p1_(t+50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>20090515.0</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>...</td>\n",
       "      <td>512.965645</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>20090519.0</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>...</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20090520.0</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>...</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>20090521.0</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>474.348364</td>\n",
       "      <td>...</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>20090522.0</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>474.348364</td>\n",
       "      <td>493.223915</td>\n",
       "      <td>...</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "      <td>560.517007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_t   p1_(t-99)   p1_(t-98)   p1_(t-97)   p1_(t-96)   p1_(t-95)  \\\n",
       "100  20090515.0  457.032497  465.530103  482.060575  472.395859  471.922241   \n",
       "101  20090519.0  465.530103  482.060575  472.395859  471.922241  472.073147   \n",
       "102  20090520.0  482.060575  472.395859  471.922241  472.073147  472.134908   \n",
       "103  20090521.0  472.395859  471.922241  472.073147  472.134908  472.519568   \n",
       "104  20090522.0  471.922241  472.073147  472.134908  472.519568  473.139085   \n",
       "\n",
       "      p1_(t-94)   p1_(t-93)   p1_(t-92)   p1_(t-91)     ...       p1_(t+41)  \\\n",
       "100  472.073147  472.134908  472.519568  473.139085     ...      512.965645   \n",
       "101  472.134908  472.519568  473.139085  467.806318     ...      516.764439   \n",
       "102  472.519568  473.139085  467.806318  468.585995     ...      516.749309   \n",
       "103  473.139085  467.806318  468.585995  474.348364     ...      516.772004   \n",
       "104  467.806318  468.585995  474.348364  493.223915     ...      519.259329   \n",
       "\n",
       "      p1_(t+42)   p1_(t+43)   p1_(t+44)   p1_(t+45)   p1_(t+46)   p1_(t+47)  \\\n",
       "100  516.764439  516.749309  516.772004  519.259329  519.221323  519.236525   \n",
       "101  516.749309  516.772004  519.259329  519.221323  519.236525  525.538546   \n",
       "102  516.772004  519.259329  519.221323  519.236525  525.538546  525.469310   \n",
       "103  519.259329  519.221323  519.236525  525.538546  525.469310  527.917451   \n",
       "104  519.221323  519.236525  525.538546  525.469310  527.917451  556.722797   \n",
       "\n",
       "      p1_(t+48)   p1_(t+49)   p1_(t+50)  \n",
       "100  525.538546  525.469310  527.917451  \n",
       "101  525.469310  527.917451  556.722797  \n",
       "102  527.917451  556.722797  556.771695  \n",
       "103  556.722797  556.771695  556.804299  \n",
       "104  556.771695  556.804299  560.517007  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "selected_features = top_correlated(df, 50, 0.75)\n",
    "selected_features = selected_features.insert(0, 'date')\n",
    "\n",
    "df_ref = reformat_as_sequence(df, 100, 50)\n",
    "df_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_memory = len(selected_features) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_bins(df, lag, block=None):\n",
    "    \"\"\"Return categories of trend (-1, 0, +1) over the last lag days at each date\"\"\"\n",
    "    # compute slopes \n",
    "    slopes = []\n",
    "    for t in range(lag, len(df)):\n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(np.arange(lag).reshape(-1, 1), df.iloc[(t-lag):t].values)\n",
    "        slopes.append(list(linreg.coef_.flatten()))\n",
    "    df_slopes = pd.DataFrame(slopes, columns=df.columns)\n",
    "    \n",
    "    # categorize slopes\n",
    "    df_copy_slopes = df_slopes.copy()\n",
    "    for col in df.columns:\n",
    "        if col not in block:\n",
    "            thre_low = np.quantile(df_copy_slopes[col].values, 0.33)\n",
    "            thre_high = np.quantile(df_copy_slopes[col].values, 0.66)\n",
    "            df_slopes[col] = 0\n",
    "            df_slopes[col][df_copy_slopes[col] >= thre_high] = 1\n",
    "            df_slopes[col][df_copy_slopes[col] <= thre_low] = -1\n",
    "            \n",
    "    df_slopes = df_slopes.drop(block, axis=1)\n",
    "    for col in block:\n",
    "        df_slopes[col] = df[col].values[lag:len(df)]\n",
    "        \n",
    "    return df_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# prepare for join\n",
    "df_trends = trend_bins(df[selected_features], 50, block=['date'])\n",
    "df_trends = df_trends.set_index('date')\n",
    "df_ref = df_ref.set_index('date_t')\n",
    "df_trends.index.astype('float64')\n",
    "\n",
    "# join\n",
    "df_boosted = df_trends.join(df_ref, how='inner')\n",
    "df_boosted = df_boosted.reset_index()\n",
    "df_boosted = df_boosted.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_boosted(df, batch_size, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"Get a batch from the dataframe. \n",
    "    \n",
    "    Each batch contains batch_size sequences. \n",
    "    Each sequences is made of input_seq_len values and the follwing output_seq_len \n",
    "    values of the time series.\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    n = df.shape[0]\n",
    "    np.random.seed(random_state)\n",
    "    rs = np.random.randint(0, n-df.shape[1], batch_size)\n",
    "    for _, r in zip(range(batch_size), rs):\n",
    "        X_batch.append(df_boosted.iloc[r, :-output_seq_len].values.reshape(-1, 1))\n",
    "        y_batch.append(df_boosted.iloc[r, -output_seq_len:].values.reshape(-1, 1))\n",
    "    X_batch = np.array(X_batch)\n",
    "    X_batch = np.array(X_batch).transpose((1, 0, 2))\n",
    "    y_batch = np.array(y_batch).transpose((1, 0, 2))\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  0.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  0.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[559.7102754],\n",
       "         [485.689497 ]],\n",
       " \n",
       "        [[562.6184014],\n",
       "         [485.7535252]],\n",
       " \n",
       "        [[562.6904985],\n",
       "         [485.7108378]],\n",
       " \n",
       "        [[563.8518416],\n",
       "         [488.2359547]],\n",
       " \n",
       "        [[566.6813071],\n",
       "         [494.5171197]],\n",
       " \n",
       "        [[566.9634645],\n",
       "         [494.5968116]],\n",
       " \n",
       "        [[565.6633556],\n",
       "         [494.5678297]],\n",
       " \n",
       "        [[563.5751894],\n",
       "         [498.3240411]],\n",
       " \n",
       "        [[561.9785593],\n",
       "         [498.199971 ]],\n",
       " \n",
       "        [[560.7067922],\n",
       "         [498.2364558]],\n",
       " \n",
       "        [[556.7637054],\n",
       "         [498.199971 ]],\n",
       " \n",
       "        [[557.094426 ],\n",
       "         [498.170787 ]],\n",
       " \n",
       "        [[558.4934783],\n",
       "         [498.2145642]],\n",
       " \n",
       "        [[558.7715812],\n",
       "         [497.9738844]],\n",
       " \n",
       "        [[559.0589458],\n",
       "         [498.0905486]],\n",
       " \n",
       "        [[554.4836453],\n",
       "         [504.5696103]],\n",
       " \n",
       "        [[554.412423 ],\n",
       "         [504.4513766]],\n",
       " \n",
       "        [[554.5459799],\n",
       "         [504.5326563]],\n",
       " \n",
       "        [[554.4391292],\n",
       "         [504.5104865]],\n",
       " \n",
       "        [[554.6439625],\n",
       "         [504.4587646]],\n",
       " \n",
       "        [[554.670691 ],\n",
       "         [504.562219 ]],\n",
       " \n",
       "        [[555.784731 ],\n",
       "         [504.3036624]],\n",
       " \n",
       "        [[561.1027906],\n",
       "         [504.4957077]],\n",
       " \n",
       "        [[562.5017664],\n",
       "         [504.4957077]],\n",
       " \n",
       "        [[562.3662071],\n",
       "         [504.4883187]],\n",
       " \n",
       "        [[561.9433889],\n",
       "         [506.9919828]],\n",
       " \n",
       "        [[566.3994304],\n",
       "         [513.2135593]],\n",
       " \n",
       "        [[576.5105501],\n",
       "         [513.2586605]],\n",
       " \n",
       "        [[583.797082 ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[582.4266663],\n",
       "         [513.1008411]],\n",
       " \n",
       "        [[583.0343053],\n",
       "         [513.2210756]],\n",
       " \n",
       "        [[583.0436635],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[583.9375236],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[583.8719758],\n",
       "         [516.8098343]],\n",
       " \n",
       "        [[589.8935324],\n",
       "         [519.3961938]],\n",
       " \n",
       "        [[593.9900009],\n",
       "         [519.5559612]],\n",
       " \n",
       "        [[594.0758156],\n",
       "         [523.3271185]],\n",
       " \n",
       "        [[594.6007778],\n",
       "         [523.250472 ]],\n",
       " \n",
       "        [[594.4002282],\n",
       "         [523.1815093]],\n",
       " \n",
       "        [[594.638993 ],\n",
       "         [523.2734636]],\n",
       " \n",
       "        [[593.9910382],\n",
       "         [525.7848654]],\n",
       " \n",
       "        [[593.6949595],\n",
       "         [525.7309632]],\n",
       " \n",
       "        [[591.4789615],\n",
       "         [525.7540628]],\n",
       " \n",
       "        [[589.8281455],\n",
       "         [525.7155646]],\n",
       " \n",
       "        [[589.8755899],\n",
       "         [519.4798692]],\n",
       " \n",
       "        [[587.9992378],\n",
       "         [519.5331313]],\n",
       " \n",
       "        [[588.1410428],\n",
       "         [515.7170401]],\n",
       " \n",
       "        [[589.6858583],\n",
       "         [515.815246 ]],\n",
       " \n",
       "        [[591.0604174],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[593.5708853],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[594.7441103],\n",
       "         [513.3263271]],\n",
       " \n",
       "        [[594.581672 ],\n",
       "         [513.2361089]],\n",
       " \n",
       "        [[594.944892 ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[595.1075289],\n",
       "         [513.3112885]],\n",
       " \n",
       "        [[594.2856892],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[593.7708094],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[594.0948889],\n",
       "         [513.3338467]],\n",
       " \n",
       "        [[593.87562  ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[593.7993904],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[599.6747791],\n",
       "         [513.3263271]],\n",
       " \n",
       "        [[596.7835829],\n",
       "         [513.2436259]],\n",
       " \n",
       "        [[597.4246067],\n",
       "         [512.9806629]],\n",
       " \n",
       "        [[593.0381702],\n",
       "         [512.6504629]],\n",
       " \n",
       "        [[586.6978329],\n",
       "         [518.1522241]],\n",
       " \n",
       "        [[582.6976102],\n",
       "         [514.4954396]],\n",
       " \n",
       "        [[583.1091793],\n",
       "         [514.55573  ]],\n",
       " \n",
       "        [[583.3151819],\n",
       "         [514.4879043]],\n",
       " \n",
       "        [[583.3151819],\n",
       "         [519.4950858]],\n",
       " \n",
       "        [[582.3822235],\n",
       "         [523.2887924]],\n",
       " \n",
       "        [[580.7753474],\n",
       "         [523.2734636]],\n",
       " \n",
       "        [[579.7842159],\n",
       "         [525.831076 ]],\n",
       " \n",
       "        [[580.0546159],\n",
       "         [525.8695911]],\n",
       " \n",
       "        [[574.362492 ],\n",
       "         [525.8541844]],\n",
       " \n",
       "        [[574.8708019],\n",
       "         [532.044209 ]],\n",
       " \n",
       "        [[563.3595826],\n",
       "         [532.0520019]],\n",
       " \n",
       "        [[563.7324256],\n",
       "         [532.1611264]],\n",
       " \n",
       "        [[558.4482072],\n",
       "         [532.0286239]],\n",
       " \n",
       "        [[563.7959141],\n",
       "         [538.4455103]],\n",
       " \n",
       "        [[563.9773886],\n",
       "         [542.0909185]],\n",
       " \n",
       "        [[559.6435662],\n",
       "         [542.0988591]],\n",
       " \n",
       "        [[559.6615739],\n",
       "         [542.0909185]],\n",
       " \n",
       "        [[561.3798717],\n",
       "         [542.1544492]],\n",
       " \n",
       "        [[561.1540298],\n",
       "         [550.8948598]],\n",
       " \n",
       "        [[561.3437248],\n",
       "         [550.8061047]],\n",
       " \n",
       "        [[560.103125 ],\n",
       "         [552.1630707]],\n",
       " \n",
       "        [[555.9477464],\n",
       "         [552.1388054]],\n",
       " \n",
       "        [[554.3039016],\n",
       "         [553.2692517]],\n",
       " \n",
       "        [[550.7093136],\n",
       "         [560.9193468]],\n",
       " \n",
       "        [[551.5444486],\n",
       "         [560.9686526]],\n",
       " \n",
       "        [[552.6647468],\n",
       "         [560.9604344]],\n",
       " \n",
       "        [[552.5043811],\n",
       "         [576.0114762]],\n",
       " \n",
       "        [[552.5845523],\n",
       "         [576.0199153]],\n",
       " \n",
       "        [[554.7138256],\n",
       "         [579.7850613]],\n",
       " \n",
       "        [[554.5079218],\n",
       "         [579.8955111]],\n",
       " \n",
       "        [[550.760899 ],\n",
       "         [582.4602067]],\n",
       " \n",
       "        [[548.4696591],\n",
       "         [584.6826419]],\n",
       " \n",
       "        [[548.8156296],\n",
       "         [588.5334648]],\n",
       " \n",
       "        [[548.3556186],\n",
       "         [586.011896 ]],\n",
       " \n",
       "        [[546.1744902],\n",
       "         [585.9861414]],\n",
       " \n",
       "        [[549.2687117],\n",
       "         [586.0376529]],\n",
       " \n",
       "        [[546.0596474],\n",
       "         [585.9775571]],\n",
       " \n",
       "        [[546.3601075],\n",
       "         [586.046239 ]],\n",
       " \n",
       "        [[546.4131642],\n",
       "         [585.8659832]],\n",
       " \n",
       "        [[548.4711367],\n",
       "         [585.8831456]],\n",
       " \n",
       "        [[543.4594242],\n",
       "         [585.8659832]],\n",
       " \n",
       "        [[542.8831901],\n",
       "         [585.934639 ]],\n",
       " \n",
       "        [[543.3096065],\n",
       "         [582.2639399]],\n",
       " \n",
       "        [[543.0331554],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[542.7909224],\n",
       "         [582.2468794]],\n",
       " \n",
       "        [[540.9057914],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[534.3967829],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[529.7991144],\n",
       "         [582.2127616]],\n",
       " \n",
       "        [[521.7699103],\n",
       "         [582.1957041]],\n",
       " \n",
       "        [[521.5067289],\n",
       "         [582.2639399]],\n",
       " \n",
       "        [[518.7253597],\n",
       "         [582.1957041]],\n",
       " \n",
       "        [[523.4936574],\n",
       "         [575.7246935]],\n",
       " \n",
       "        [[520.9891774],\n",
       "         [575.7837136]],\n",
       " \n",
       "        [[518.5834809],\n",
       "         [575.8258783]],\n",
       " \n",
       "        [[518.3383077],\n",
       "         [563.3904386]],\n",
       " \n",
       "        [[511.0527729],\n",
       "         [563.3244253]],\n",
       " \n",
       "        [[511.0193789],\n",
       "         [557.0897432]],\n",
       " \n",
       "        [[513.3757398],\n",
       "         [557.0815835]],\n",
       " \n",
       "        [[515.8122237],\n",
       "         [557.0815835]],\n",
       " \n",
       "        [[512.7062685],\n",
       "         [563.2336823]],\n",
       " \n",
       "        [[512.9571155],\n",
       "         [560.5252121]],\n",
       " \n",
       "        [[517.400413 ],\n",
       "         [560.7057884]],\n",
       " \n",
       "        [[522.9788541],\n",
       "         [560.6565288]],\n",
       " \n",
       "        [[511.188054 ],\n",
       "         [560.8289753]],\n",
       " \n",
       "        [[510.0195856],\n",
       "         [557.0734242]],\n",
       " \n",
       "        [[509.9447588],\n",
       "         [550.6367426]],\n",
       " \n",
       "        [[514.4265273],\n",
       "         [550.7496391]],\n",
       " \n",
       "        [[516.3090877],\n",
       "         [550.7173782]],\n",
       " \n",
       "        [[514.6195894],\n",
       "         [547.9252585]],\n",
       " \n",
       "        [[516.1489762],\n",
       "         [548.1579266]],\n",
       " \n",
       "        [[516.0394834],\n",
       "         [548.2301741]],\n",
       " \n",
       "        [[515.6019765],\n",
       "         [547.9653596]],\n",
       " \n",
       "        [[513.2294273],\n",
       "         [547.8450739]],\n",
       " \n",
       "        [[510.4310834],\n",
       "         [548.1017472]],\n",
       " \n",
       "        [[507.5090483],\n",
       "         [548.3345652]],\n",
       " \n",
       "        [[507.3439778],\n",
       "         [550.7708906]],\n",
       " \n",
       "        [[508.6924388],\n",
       "         [549.4532376]],\n",
       " \n",
       "        [[510.0611655],\n",
       "         [549.5500277]],\n",
       " \n",
       "        [[516.2416603],\n",
       "         [546.7893285]],\n",
       " \n",
       "        [[515.6692369],\n",
       "         [547.3931064]],\n",
       " \n",
       "        [[516.9617808],\n",
       "         [547.0387285]]]), array([[[517.3244426],\n",
       "         [540.6475147]],\n",
       " \n",
       "        [[522.5099371],\n",
       "         [529.3904334]],\n",
       " \n",
       "        [[525.2024746],\n",
       "         [529.7967132]],\n",
       " \n",
       "        [[529.1834212],\n",
       "         [523.1190247]],\n",
       " \n",
       "        [[530.5760092],\n",
       "         [517.052779 ]]]))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just an example\n",
    "fetch_sample_boosted(df_boosted, 2, 10, 5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_boosted(df, trend_memory):\n",
    "    scaler = StandardScaler()\n",
    "    values = df[trend_memory:].values\n",
    "    #.reshape(-1, 1)\n",
    "    values = scaler.fit_transform(values)\n",
    "    concat = pd.concat([df.iloc[:, :trend_memory], pd.DataFrame(values)], axis=1)\n",
    "    return pd.DataFrame(values), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    scaler = StandardScaler()\n",
    "    values = df.values\n",
    "    #.reshape(-1, 1)\n",
    "    values = scaler.fit_transform(values)\n",
    "    return pd.DataFrame(values), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1955, 167)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boosted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127205</td>\n",
       "      <td>0.164588</td>\n",
       "      <td>0.164142</td>\n",
       "      <td>0.164110</td>\n",
       "      <td>0.188528</td>\n",
       "      <td>0.187886</td>\n",
       "      <td>0.187691</td>\n",
       "      <td>0.249809</td>\n",
       "      <td>0.248787</td>\n",
       "      <td>0.272714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.164438</td>\n",
       "      <td>0.164367</td>\n",
       "      <td>0.188783</td>\n",
       "      <td>0.188151</td>\n",
       "      <td>0.188036</td>\n",
       "      <td>0.250182</td>\n",
       "      <td>0.249123</td>\n",
       "      <td>0.273055</td>\n",
       "      <td>0.558208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164663</td>\n",
       "      <td>0.189043</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.188302</td>\n",
       "      <td>0.250537</td>\n",
       "      <td>0.249495</td>\n",
       "      <td>0.273394</td>\n",
       "      <td>0.558595</td>\n",
       "      <td>0.558692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164975</td>\n",
       "      <td>0.189342</td>\n",
       "      <td>0.188666</td>\n",
       "      <td>0.188557</td>\n",
       "      <td>0.250809</td>\n",
       "      <td>0.249850</td>\n",
       "      <td>0.273771</td>\n",
       "      <td>0.558980</td>\n",
       "      <td>0.559080</td>\n",
       "      <td>0.559015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189656</td>\n",
       "      <td>0.188965</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>0.251070</td>\n",
       "      <td>0.250122</td>\n",
       "      <td>0.274130</td>\n",
       "      <td>0.559405</td>\n",
       "      <td>0.559465</td>\n",
       "      <td>0.559403</td>\n",
       "      <td>0.595813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "1 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "2 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "3 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "4 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "\n",
       "        7         8         9      ...          157       158       159  \\\n",
       "0  1.225142  1.223024  1.211009    ...     0.127205  0.164588  0.164142   \n",
       "1  1.225142  1.223024  1.211009    ...     0.164900  0.164438  0.164367   \n",
       "2  1.225142  1.223024  1.211009    ...     0.164750  0.164663  0.189043   \n",
       "3  1.225142  1.223024  1.211009    ...     0.164975  0.189342  0.188666   \n",
       "4  1.225142  1.223024  1.211009    ...     0.189656  0.188965  0.188816   \n",
       "\n",
       "        160       161       162       163       164       165       166  \n",
       "0  0.164110  0.188528  0.187886  0.187691  0.249809  0.248787  0.272714  \n",
       "1  0.188783  0.188151  0.188036  0.250182  0.249123  0.273055  0.558208  \n",
       "2  0.188406  0.188302  0.250537  0.249495  0.273394  0.558595  0.558692  \n",
       "3  0.188557  0.250809  0.249850  0.273771  0.558980  0.559080  0.559015  \n",
       "4  0.251070  0.250122  0.274130  0.559405  0.559465  0.559403  0.595813  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized/scaled prices\n",
    "df_boosted.iloc[:, :trend_memory] = df_boosted.iloc[:, :trend_memory] * 500 \n",
    "df_boosted_sc, scaler = normalize(df_boosted)\n",
    "df_boosted_sc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq with GRU cells model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1564 0.8\n",
      "test 391 0.2\n"
     ]
    }
   ],
   "source": [
    "# Dataframe we work on \n",
    "df = df_boosted_sc\n",
    "\n",
    "# Proportion of samples in the training set \n",
    "train_prop = 0.8\n",
    "\n",
    "# train test split\n",
    "cut = int(train_prop * len(df))\n",
    "df_train = df[:cut]\n",
    "df_test = df[cut:]\n",
    "\n",
    "# sanity check\n",
    "print('train', len(df_train), len(df_train)/len(df))\n",
    "print('test', len(df_test), len(df_test)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths to TF seq2seq model and recurrent cells to be used in this project\n",
    "tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "tf.nn.rnn_cell = tf.contrib.rnn \n",
    "tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell # Useful for learning long-range dependencies in sequences\n",
    "\n",
    "# Data shape parameters\n",
    "batch_size = 15 # How many time series to train on before updating model's weight parameters\n",
    "output_seq_len = 50 # How many days to predict into the future\n",
    "input_seq_len = 100 + trend_memory # How many days to train on in the past + trends\n",
    "\n",
    "# Internal neural network parameters\n",
    "input_dim = output_dim = 1 # Univariate time series (predicting future values based on stream of historical values)\n",
    "hidden_dim = 50  # Number of neurons in each recurrent unit \n",
    "num_layers = 2  # Number of stacked recurrent cells (number of recurrent layers)\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.005  # Small lr helps not to diverge during training. \n",
    "epochs =  1000 #1000  # How many times we perform a training step (how many times we show a batch)\n",
    "lr_decay = 0.9  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.2  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.01  # L2 regularization of weights - reduces overfitting\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset any existing graph, close any previous session, discard old variables, and start fresh\n",
    "tf.reset_default_graph()\n",
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "with tf.variable_scope('Seq2Seq'):\n",
    "    # Input values to encoder RNN\n",
    "    encoder_inputs = [tf.placeholder(tf.float32, shape=(None, input_dim), \n",
    "                     name=\"encoder_input_{}\".format(t)) for t in range(input_seq_len)]\n",
    "    \n",
    "    # Target values for decoder RNN\n",
    "    decoder_targets = [tf.placeholder(tf.float32, shape=(None, output_dim), \n",
    "                       name=\"decoder_target_{}\".format(t)) for t in range(output_seq_len)]\n",
    "    \n",
    "    # Feed final n encoder inputs into the decoder RNN, where n = output_seq_len\n",
    "    # \"GO\", represented by 0, starts the decoder\n",
    "    decoder_inputs = [tf.zeros_like(encoder_inputs[0], dtype=np.float32, name=\"GO\")] +\\\n",
    "                      encoder_inputs[-(output_seq_len - 1):]\n",
    "    \n",
    "    # Stack hidden recurrent layers\n",
    "    cells = list()\n",
    "    for i in range(num_layers):\n",
    "        with tf.variable_scope('RNN_' + str(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Pass encoder and decoder inputs through model, retrieving output from the decoder at each prediction step\n",
    "    decoder_outputs, decoder_state = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "    \n",
    "    # Squeeze decoder output into a single value, representing the forecast at that point in the sequence\n",
    "    W_out = tf.Variable(tf.truncated_normal([hidden_dim, output_dim], seed=random_state)) # Output weight matrix\n",
    "    b_out = tf.Variable(tf.truncated_normal([output_dim], seed=random_state)) # Output bias\n",
    "    \n",
    "    # Apply a trainable, constant linear transformation to final outputs\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_Scale_Factor\")\n",
    "    reshaped_outputs = [output_scale_factor * (tf.matmul(i, W_out) + b_out) for i in decoder_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    # Compute Mean Absolute Percentage loss for output at each time step: \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, decoder_targets):\n",
    "        #output_loss += tf.reduce_mean(tf.metrics.mean_absolute_error(_Y, _y))\n",
    "        #output_loss += tf.reduce_mean(tf.abs((_Y-_y)/_Y))\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "    # Penalize model complexity with L2 regularization\n",
    "    output_loss = output_loss / len(reshaped_outputs)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "    # Add regularization term to loss function        \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    # Search for minimum of loss function with RMSProp:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum, centered=False)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(df, batch_size, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    Trains session model, attempting to optimize internal weight parameters\n",
    "    to accurately predict the number of steps into future given by output_seq_len\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to reserve for prediction (training target)\n",
    "    \"\"\"\n",
    "    X_train, y_train = fetch_sample_boosted(df=df, \n",
    "                                    batch_size=batch_size, \n",
    "                                    input_seq_len=input_seq_len, \n",
    "                                    output_seq_len=output_seq_len,\n",
    "                                    )\n",
    "    feed_dict = {encoder_inputs[t]: X_train[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_train[t] for t in range(len(decoder_targets))})\n",
    "    train_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(df, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"\n",
    "    Tests session model on a batch of random time series drawn from one of the metrics DataFrames.\n",
    "    All passed parameters should be same as those used during training.\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to set aside for prediction (training target)\n",
    "    @random_state: Controls reproducible output\n",
    "    \"\"\"\n",
    "    X_test, y_test = fetch_sample_boosted(df=df, \n",
    "                                  batch_size=1, \n",
    "                                  input_seq_len=input_seq_len, \n",
    "                                  output_seq_len=output_seq_len,\n",
    "                                  random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    test_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return test_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/1000 \ttrain loss: 2105634.5 \tdev loss: 111129.671875\n",
      "Step 100/1000 \ttrain loss: 1282810.0 \tdev loss: 75858.875\n",
      "Step 200/1000 \ttrain loss: 723446.0625 \tdev loss: 51444.2421875\n",
      "Step 300/1000 \ttrain loss: 207632.78125 \tdev loss: 8336.5087890625\n",
      "Step 400/1000 \ttrain loss: 33213.47265625 \tdev loss: 462.39324951171875\n",
      "Step 500/1000 \ttrain loss: 101033.171875 \tdev loss: 2290.2822265625\n",
      "Step 600/1000 \ttrain loss: 20993.87890625 \tdev loss: 356.86163330078125\n",
      "Step 700/1000 \ttrain loss: 90557.2734375 \tdev loss: 2610.27490234375\n",
      "Step 800/1000 \ttrain loss: 91072.6015625 \tdev loss: 5161.712890625\n",
      "Step 900/1000 \ttrain loss: 29756.359375 \tdev loss: 628.3524169921875\n",
      "Step 1000/1000 \ttrain loss: 20005.623046875 \tdev loss: 1076.1888427734375\n"
     ]
    }
   ],
   "source": [
    "# Reset variables and run passengers training ops\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(epochs + 1):\n",
    "    train_loss = train_batch(df=df_train, batch_size=batch_size, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "    # Taking the dev_loss on the same random samples serves as a validation run every 100 training runs\n",
    "    if t % 100 == 0:\n",
    "        dev_loss = test_batch(df=df_test, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "        print(\"Step {0}/{1} \\ttrain loss: {2} \\tdev loss: {3}\".format(t, epochs, train_loss[1], dev_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then every time you have specified a graph run:\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-500.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[   0.       ]]\n",
      "\n",
      " [[ 500.       ]]\n",
      "\n",
      " [[ 450.8642504]]\n",
      "\n",
      " [[ 448.3266022]]\n",
      "\n",
      " [[ 444.5957322]]\n",
      "\n",
      " [[ 444.6413391]]\n",
      "\n",
      " [[ 447.8148138]]\n",
      "\n",
      " [[ 454.3751066]]\n",
      "\n",
      " [[ 454.4816018]]\n",
      "\n",
      " [[ 454.2088077]]\n",
      "\n",
      " [[ 454.2021582]]\n",
      "\n",
      " [[ 454.0359859]]\n",
      "\n",
      " [[ 462.8118195]]\n",
      "\n",
      " [[ 462.5207556]]\n",
      "\n",
      " [[ 462.6493201]]\n",
      "\n",
      " [[ 466.5506919]]\n",
      "\n",
      " [[ 468.805319 ]]\n",
      "\n",
      " [[ 468.9081979]]\n",
      "\n",
      " [[ 475.0908017]]\n",
      "\n",
      " [[ 475.2506947]]\n",
      "\n",
      " [[ 481.2997544]]\n",
      "\n",
      " [[ 481.4546784]]\n",
      "\n",
      " [[ 481.3419965]]\n",
      "\n",
      " [[ 481.3983309]]\n",
      "\n",
      " [[ 481.4546784]]\n",
      "\n",
      " [[ 485.3838173]]\n",
      "\n",
      " [[ 485.2133726]]\n",
      "\n",
      " [[ 485.2914782]]\n",
      "\n",
      " [[ 481.5392245]]\n",
      "\n",
      " [[ 481.4969477]]\n",
      "\n",
      " [[ 481.6097022]]\n",
      "\n",
      " [[ 481.4758122]]\n",
      "\n",
      " [[ 481.595605 ]]\n",
      "\n",
      " [[ 487.8429536]]\n",
      "\n",
      " [[ 494.1696736]]\n",
      "\n",
      " [[ 494.1407418]]\n",
      "\n",
      " [[ 494.1552073]]\n",
      "\n",
      " [[ 494.0973504]]\n",
      "\n",
      " [[ 500.432311 ]]\n",
      "\n",
      " [[ 506.7396372]]\n",
      "\n",
      " [[ 512.9656446]]\n",
      "\n",
      " [[ 516.764439 ]]\n",
      "\n",
      " [[ 516.749309 ]]\n",
      "\n",
      " [[ 516.7720043]]\n",
      "\n",
      " [[ 519.2593285]]\n",
      "\n",
      " [[ 519.2213232]]\n",
      "\n",
      " [[ 519.2365246]]\n",
      "\n",
      " [[ 525.5385456]]\n",
      "\n",
      " [[ 525.4693098]]\n",
      "\n",
      " [[ 527.9174507]]\n",
      "\n",
      " [[ 556.7227968]]\n",
      "\n",
      " [[ 556.7716951]]\n",
      "\n",
      " [[ 556.8042987]]\n",
      "\n",
      " [[ 560.5170069]]\n",
      "\n",
      " [[ 556.7227968]]\n",
      "\n",
      " [[ 544.0130983]]\n",
      "\n",
      " [[ 537.6184777]]\n",
      "\n",
      " [[ 537.705    ]]\n",
      "\n",
      " [[ 537.8151597]]\n",
      "\n",
      " [[ 525.2463405]]\n",
      "\n",
      " [[ 518.8567548]]\n",
      "\n",
      " [[ 506.5245996]]\n",
      "\n",
      " [[ 506.5097762]]\n",
      "\n",
      " [[ 506.6802982]]\n",
      "\n",
      " [[ 506.7173834]]\n",
      "\n",
      " [[ 506.7248011]]\n",
      "\n",
      " [[ 506.7025487]]\n",
      "\n",
      " [[ 506.7248011]]\n",
      "\n",
      " [[ 506.6802982]]\n",
      "\n",
      " [[ 503.0301938]]\n",
      "\n",
      " [[ 503.0449234]]\n",
      "\n",
      " [[ 494.2420179]]\n",
      "\n",
      " [[ 494.278198 ]]\n",
      "\n",
      " [[ 488.114414 ]]\n",
      "\n",
      " [[ 488.2145019]]\n",
      "\n",
      " [[ 488.1287098]]\n",
      "\n",
      " [[ 488.1001191]]\n",
      "\n",
      " [[ 488.150155 ]]\n",
      "\n",
      " [[ 481.8635931]]\n",
      "\n",
      " [[ 481.8635931]]\n",
      "\n",
      " [[ 475.8634542]]\n",
      "\n",
      " [[ 475.7728335]]\n",
      "\n",
      " [[ 475.6613476]]\n",
      "\n",
      " [[ 473.1024368]]\n",
      "\n",
      " [[ 473.2410363]]\n",
      "\n",
      " [[ 473.2271727]]\n",
      "\n",
      " [[ 473.2479684]]\n",
      "\n",
      " [[ 473.1370791]]\n",
      "\n",
      " [[ 473.2341044]]\n",
      "\n",
      " [[ 460.6472093]]\n",
      "\n",
      " [[ 460.7551777]]\n",
      "\n",
      " [[ 460.9645106]]\n",
      "\n",
      " [[ 460.7416789]]\n",
      "\n",
      " [[ 460.7619274]]\n",
      "\n",
      " [[ 460.680944 ]]\n",
      "\n",
      " [[ 475.7588949]]\n",
      "\n",
      " [[ 475.6892142]]\n",
      "\n",
      " [[ 475.7728335]]\n",
      "\n",
      " [[ 481.9059342]]\n",
      "\n",
      " [[ 481.8918197]]\n",
      "\n",
      " [[ 485.6254856]]\n",
      "\n",
      " [[ 485.5828208]]]\n",
      "[[array([[559.2042]], dtype=float32), array([[543.67255]], dtype=float32), array([[541.5752]], dtype=float32), array([[541.35803]], dtype=float32), array([[541.33575]], dtype=float32), array([[541.33307]], dtype=float32), array([[541.33215]], dtype=float32), array([[541.33167]], dtype=float32), array([[541.331]], dtype=float32), array([[541.3304]], dtype=float32), array([[541.3299]], dtype=float32), array([[541.3292]], dtype=float32), array([[541.32874]], dtype=float32), array([[541.32825]], dtype=float32), array([[541.32764]], dtype=float32), array([[541.3271]], dtype=float32), array([[541.32654]], dtype=float32), array([[541.32605]], dtype=float32), array([[541.32556]], dtype=float32), array([[541.325]], dtype=float32), array([[541.32434]], dtype=float32), array([[541.3239]], dtype=float32), array([[541.3234]], dtype=float32), array([[541.3228]], dtype=float32), array([[541.3223]], dtype=float32), array([[541.3218]], dtype=float32), array([[541.3212]], dtype=float32), array([[541.3208]], dtype=float32), array([[541.3203]], dtype=float32), array([[541.31964]], dtype=float32), array([[541.31903]], dtype=float32), array([[541.3188]], dtype=float32), array([[541.3181]], dtype=float32), array([[541.3176]], dtype=float32), array([[541.31714]], dtype=float32), array([[541.31665]], dtype=float32), array([[541.31616]], dtype=float32), array([[541.3157]], dtype=float32), array([[541.3151]], dtype=float32), array([[541.31464]], dtype=float32), array([[541.3143]], dtype=float32), array([[541.31366]], dtype=float32), array([[541.3131]], dtype=float32), array([[541.3126]], dtype=float32), array([[541.31226]], dtype=float32), array([[541.3117]], dtype=float32), array([[541.3112]], dtype=float32), array([[541.3108]], dtype=float32), array([[541.3103]], dtype=float32), array([[541.3098]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = fetch_sample_boosted(df_test, 1, input_seq_len, output_seq_len, random_state=random_state)\n",
    "feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "res = sess.run([reshaped_outputs], feed_dict=feed_dict)\n",
    "print(X_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True,\n",
       " 'mean_': array([ 21.99488491,  19.43734015,   0.76726343,  20.97186701,\n",
       "         -1.79028133,   3.58056266,  -2.81329923,   3.58056266,\n",
       "          0.76726343,   8.4398977 ,  11.76470588,  12.27621483,\n",
       "         12.78772379,  11.25319693,   4.34782609,   4.09207161,\n",
       "          3.58056266, 493.51417388, 493.57327903, 493.62541598,\n",
       "        493.66358313, 493.70008178, 493.7318277 , 493.76692478,\n",
       "        493.80414752, 493.84078244, 493.87674821, 493.9184831 ,\n",
       "        493.96166305, 494.00336601, 494.03980984, 494.07925284,\n",
       "        494.11830452, 494.15430546, 494.18549471, 494.21773926,\n",
       "        494.24831   , 494.27069688, 494.29050425, 494.30475539,\n",
       "        494.31350792, 494.32224932, 494.33402724, 494.34427521,\n",
       "        494.35486302, 494.36798506, 494.38438908, 494.39960453,\n",
       "        494.41341702, 494.42829549, 494.44458952, 494.46104261,\n",
       "        494.47857125, 494.49572636, 494.51134567, 494.52838872,\n",
       "        494.54697953, 494.57631189, 494.60888633, 494.64599845,\n",
       "        494.68552972, 494.73619239, 494.78841242, 494.85541052,\n",
       "        494.92041345, 494.98128101, 495.03931932, 495.10182075,\n",
       "        495.16789731, 495.23402844, 495.30431672, 495.3773666 ,\n",
       "        495.45079109, 495.52787505, 495.60203097, 495.67502759,\n",
       "        495.74996393, 495.82607793, 495.89398627, 495.96057534,\n",
       "        496.02747895, 496.09553258, 496.16222909, 496.23122963,\n",
       "        496.29820402, 496.36575482, 496.43255902, 496.49833197,\n",
       "        496.56442798, 496.62676196, 496.68972668, 496.75209225,\n",
       "        496.80943577, 496.86781763, 496.93238915, 496.99611448,\n",
       "        497.06150463, 497.13552404, 497.21007689, 497.28472192,\n",
       "        497.35914752, 497.42760557, 497.4974446 , 497.56916435,\n",
       "        497.64246914, 497.71542648, 497.78857012, 497.86260209,\n",
       "        497.93489244, 498.00800312, 498.07529319, 498.14177389,\n",
       "        498.20752774, 498.2712253 , 498.34106495, 498.41122577,\n",
       "        498.48176159, 498.55245382, 498.62046248, 498.68377815,\n",
       "        498.74600487, 498.80852202, 498.87233373, 498.93014806,\n",
       "        498.98852898, 499.04222196, 499.09598425, 499.15140354,\n",
       "        499.20482732, 499.25795575, 499.30426652, 499.34903988,\n",
       "        499.39527641, 499.42610776, 499.45268152, 499.48027471,\n",
       "        499.50721792, 499.53380276, 499.56036506, 499.58574937,\n",
       "        499.6112666 , 499.63838691, 499.66334617, 499.69021405,\n",
       "        499.71657697, 499.74314737, 499.77386475, 499.80706651,\n",
       "        499.83949211, 499.8719417 , 499.91064939, 499.94670134,\n",
       "        499.97851347, 500.01071506, 500.04139465, 500.0815507 ,\n",
       "        500.11571084, 500.14626492, 500.17569422, 500.20354153,\n",
       "        500.22787992, 500.25182065, 500.27655152, 500.30841848,\n",
       "        500.34173911, 500.37159366, 500.40160715]),\n",
       " 'n_samples_seen_': 1955,\n",
       " 'scale_': array([408.69965332, 405.6893228 , 408.19536037, 405.92803808,\n",
       "        405.99329364, 405.19322918, 405.35704754, 405.19322918,\n",
       "        408.19536037, 405.90950681, 407.86977748, 410.04358252,\n",
       "        410.02794921, 406.94258295, 413.77375206, 413.62180696,\n",
       "        411.45673222,  99.6806871 ,  99.69329471,  99.70526728,\n",
       "         99.71511411,  99.72038166,  99.72332251,  99.72776788,\n",
       "         99.73326732,  99.73859629,  99.74380209,  99.7499546 ,\n",
       "         99.75725026,  99.76608335,  99.77880429,  99.79405353,\n",
       "         99.80913423,  99.82375983,  99.83478508,  99.84876482,\n",
       "         99.86142058,  99.872065  ,  99.88086456,  99.88635292,\n",
       "         99.88927151,  99.8921766 ,  99.8964323 ,  99.89997193,\n",
       "         99.90368222,  99.90859621,  99.9152445 ,  99.92123136,\n",
       "         99.92642714,  99.93219128,  99.93875371,  99.94646317,\n",
       "         99.95468921,  99.96276573,  99.9699595 ,  99.97801667,\n",
       "         99.98710566,  99.9988998 , 100.01338031, 100.02895376,\n",
       "        100.04640686, 100.0651024 , 100.08474638, 100.10693425,\n",
       "        100.13131182, 100.15156734, 100.16928837, 100.18709366,\n",
       "        100.20821167, 100.22920485, 100.24534499, 100.2596233 ,\n",
       "        100.27407812, 100.29206724, 100.31185533, 100.33042832,\n",
       "        100.35081265, 100.37237364, 100.38620291, 100.39889231,\n",
       "        100.4118048 , 100.42572051, 100.43851864, 100.44940932,\n",
       "        100.4585048 , 100.46774894, 100.4765288 , 100.48440955,\n",
       "        100.49242429, 100.49768576, 100.50316402, 100.50829236,\n",
       "        100.51014301, 100.51257175, 100.51507603, 100.51691337,\n",
       "        100.51823013, 100.51668229, 100.51559257, 100.51450381,\n",
       "        100.51322256, 100.5139652 , 100.51556882, 100.51882411,\n",
       "        100.52304833, 100.52709507, 100.53124053, 100.53414504,\n",
       "        100.5357225 , 100.53795332, 100.54227715, 100.54595167,\n",
       "        100.54912431, 100.55312595, 100.5633815 , 100.57375785,\n",
       "        100.58448773, 100.59525224, 100.60372081, 100.61249662,\n",
       "        100.62052167, 100.62714459, 100.63230081, 100.63359552,\n",
       "        100.63703149, 100.64121676, 100.64547162, 100.65056901,\n",
       "        100.65441489, 100.65797115, 100.66201955, 100.66511526,\n",
       "        100.66900709, 100.66817332, 100.66694366, 100.66596074,\n",
       "        100.66647839, 100.66693179, 100.66896809, 100.6706562 ,\n",
       "        100.67235105, 100.67458256, 100.67612002, 100.67931456,\n",
       "        100.68226813, 100.68531199, 100.68891403, 100.69358352,\n",
       "        100.69792508, 100.70222362, 100.70973449, 100.71802317,\n",
       "        100.72601503, 100.73420629, 100.74155154, 100.75481958,\n",
       "        100.7662532 , 100.77731253, 100.78945172, 100.80155136,\n",
       "        100.81128715, 100.82077031, 100.83135905, 100.84718482,\n",
       "        100.86419352, 100.88028351, 100.8964733 ]),\n",
       " 'var_': array([167035.40662345, 164583.8266364 , 166623.45222755, 164777.57209856,\n",
       "        164830.55448355, 164181.55297257, 164314.33598681, 164181.55297257,\n",
       "        166623.45222755, 164762.52771764, 166357.75537837, 168135.73956214,\n",
       "        168122.91913318, 165602.26581459, 171208.71789169, 171082.99919545,\n",
       "        169296.64248664,   9936.23938128,   9938.75301064,   9941.14032341,\n",
       "          9943.10398187,   9944.1545191 ,   9944.74105212,   9945.62768587,\n",
       "          9946.72461039,   9947.78759053,   9948.82605556,   9950.05344249,\n",
       "          9951.50897944,   9953.27138777,   9955.80978653,   9958.8531195 ,\n",
       "          9961.86327523,   9964.78302753,   9966.98431199,   9969.7758351 ,\n",
       "          9972.30332053,   9974.42936753,   9976.18710605,   9977.28349918,\n",
       "          9977.86656211,   9978.44694688,   9979.29718695,   9980.00439163,\n",
       "          9980.74572106,   9981.72759644,   9983.05608302,   9984.25247698,\n",
       "          9985.29084192,   9986.44285451,   9987.75449386,   9989.29550093,\n",
       "          9990.93989539,   9992.55453157,   9993.9928017 ,   9995.60381674,\n",
       "          9997.421298  ,   9999.77996116,  10002.6762418 ,  10005.79158974,\n",
       "         10009.28352529,  10013.02471808,  10016.95645891,  10021.39828532,\n",
       "         10026.27960719,  10030.33643979,  10033.88633328,  10037.45373636,\n",
       "         10041.68568703,  10045.89350478,  10049.12919313,  10051.99206401,\n",
       "         10054.89074321,  10058.4987504 ,  10062.46831919,  10066.19484729,\n",
       "         10070.28559851,  10074.6133899 ,  10077.38973505,  10079.93757676,\n",
       "         10082.53054355,  10085.32534052,  10087.89602652,  10090.08383238,\n",
       "         10091.91118679,  10093.76857725,  10095.53283915,  10097.11656166,\n",
       "         10098.72733948,  10099.78484242,  10100.88597852,  10101.91683394,\n",
       "         10102.28884839,  10102.77708023,  10103.280509  ,  10103.64987257,\n",
       "         10103.91458907,  10103.60341928,  10103.38434909,  10103.16547688,\n",
       "         10102.90790957,  10103.05720045,  10103.37957536,  10104.03399975,\n",
       "         10104.88324472,  10105.69684238,  10106.53032249,  10107.1143199 ,\n",
       "         10107.43149786,  10107.88005832,  10108.74949465,  10109.48839792,\n",
       "         10110.12639971,  10110.9311376 ,  10112.99369954,  10115.08076862,\n",
       "         10117.23917097,  10119.40477369,  10121.10864015,  10122.87447655,\n",
       "         10124.48938038,  10125.82222772,  10126.85996597,  10127.12054799,\n",
       "         10127.81210738,  10128.65450995,  10129.51095781,  10130.53704158,\n",
       "         10131.31123634,  10132.0271563 ,  10132.84217892,  10133.46542942,\n",
       "         10134.24898751,  10134.08111965,  10133.83354655,  10133.63565186,\n",
       "         10133.73987072,  10133.8311559 ,  10134.24113535,  10134.58102038,\n",
       "         10134.92226611,  10135.37157432,  10135.68114315,  10136.32438102,\n",
       "         10136.91911495,  10137.53205024,  10138.25740765,  10139.19776191,\n",
       "         10140.07211444,  10140.93784199,  10142.45062082,  10144.12019102,\n",
       "         10145.7301032 ,  10147.38031705,  10148.86020615,  10151.53366775,\n",
       "         10153.83778479,  10156.06672075,  10158.5135785 ,  10160.95275669,\n",
       "         10162.91561761,  10164.8277263 ,  10166.96296725,  10170.15468521,\n",
       "         10173.58553417,  10176.83160206,  10180.09832415]),\n",
       " 'with_mean': True,\n",
       " 'with_std': True}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-500.           0.           0.           0.           0.\n",
      "  500.           0.           0.           0.           0.\n",
      "    0.         500.         500.         500.           0.\n",
      "    0.         500.         442.2918738  444.8042974  444.6674044\n",
      "  444.7977768  444.732581   444.6348233  450.7057489  450.8642504\n",
      "  448.3266022  444.5957322  444.6413391  447.8148138  454.3751066\n",
      "  454.4816018  454.2088077  454.2021582  454.0359859  462.8118195\n",
      "  462.5207556  462.6493201  466.5506919  468.805319   468.9081979\n",
      "  475.0908017  475.2506947  481.2997544  481.4546784  481.3419965\n",
      "  481.3983309  481.4546784  485.3838173  485.2133726  485.2914782\n",
      "  481.5392245  481.4969477  481.6097022  481.4758122  481.595605\n",
      "  487.8429536  494.1696736  494.1407418  494.1552073  494.0973504\n",
      "  500.432311   506.7396372  512.9656446  516.764439   516.749309\n",
      "  516.7720043  519.2593285  519.2213232  519.2365246  525.5385456\n",
      "  525.4693098  527.9174507  556.7227968  556.7716951  556.8042987\n",
      "  560.5170069  556.7227968  544.0130983  537.6184777  537.705\n",
      "  537.8151597  525.2463405  518.8567548  506.5245996  506.5097762\n",
      "  506.6802982  506.7173834  506.7248011  506.7025487  506.7248011\n",
      "  506.6802982  503.0301938  503.0449234  494.2420179  494.278198\n",
      "  488.114414   488.2145019  488.1287098  488.1001191  488.150155\n",
      "  481.8635931  481.8635931  475.8634542  475.7728335  475.6613476\n",
      "  473.1024368  473.2410363  473.2271727  473.2479684  473.1370791\n",
      "  473.2341044  460.6472093  460.7551777  460.9645106  460.7416789\n",
      "  460.7619274  460.680944 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (117,) (167,) (117,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-fb0be56710e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviz_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAPE on sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_mape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-319-fb0be56710e2>\u001b[0m in \u001b[0;36mviz_prediction\u001b[0;34m(X_test, y_test, batch_sample_nb)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sample_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sample_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sample_nb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (117,) (167,) (117,) "
     ]
    }
   ],
   "source": [
    "def viz_prediction(X_test, y_test, batch_sample_nb):\n",
    "    assert (batch_sample_nb < batch_size) & (batch_sample_nb >= 0)\n",
    "    X_test, y_test = fetch_sample_boosted(df_train, batch_size, input_seq_len, output_seq_len)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    print(X_test[:,2, :].flatten())\n",
    "    X_plot = list(scaler.inverse_transform(X_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_plot = list(scaler.inverse_transform(y_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_pred = list(scaler.inverse_transform(res[:, batch_sample_nb].flatten()))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(X_plot + y_pred, label='prediction')\n",
    "    plt.plot(X_plot + y_plot, label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return (y_pred, y_plot)\n",
    "    \n",
    "y_pred, y_true = viz_prediction(X_test, y_test, 0)\n",
    "print(\"MAPE on sample\", score_mape(y_pred, y_true, as_days=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 21\n",
    "# Eval on a full batch:\n",
    "def eval_batch(random_state, batch_size):\n",
    "    X_test, y_test = fetch_sample_boosted(df_test, batch_size, input_seq_len, output_seq_len, random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    MAPE = 0\n",
    "    for b in range(batch_size):\n",
    "        X_plot = list(scaler.inverse_transform(X_test[:, b, :].flatten()))\n",
    "        y_true = list(scaler.inverse_transform(y_test[:, b, :].flatten()))\n",
    "        y_pred = list(scaler.inverse_transform(res[:, b].flatten()))\n",
    "        MAPE += score_mape(y_pred, y_true, as_days=True)\n",
    "    return MAPE / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (117,) (167,) (117,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-322-2d8593dccc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAPE on 15 batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-321-7142f7eddb00>\u001b[0m in \u001b[0;36meval_batch\u001b[0;34m(random_state, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mMAPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (117,) (167,) (117,) "
     ]
    }
   ],
   "source": [
    "print(\"MAPE on 15 batch\", eval_batch(random_state, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
