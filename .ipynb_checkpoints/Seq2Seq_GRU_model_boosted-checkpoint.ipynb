{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted GRU model for price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an encoder-decoder made of GRU cells, boosted with other features included in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tensorflow 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "print('Imported tensorflow', tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folder scripts\n",
    "from feature_selection_feed import top_correlated, reformat_as_sequence\n",
    "from evaluation import score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>f000_open</th>\n",
       "      <th>f000_high</th>\n",
       "      <th>f000_low</th>\n",
       "      <th>f000_settle</th>\n",
       "      <th>f001_open</th>\n",
       "      <th>f001_high</th>\n",
       "      <th>...</th>\n",
       "      <th>f136_open</th>\n",
       "      <th>f136_high</th>\n",
       "      <th>f136_low</th>\n",
       "      <th>f136_settle</th>\n",
       "      <th>f137_open</th>\n",
       "      <th>f137_high</th>\n",
       "      <th>f137_low</th>\n",
       "      <th>f137_settle</th>\n",
       "      <th>week</th>\n",
       "      <th>week_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20081201</td>\n",
       "      <td>444.511058</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>53.08</td>\n",
       "      <td>56.33</td>\n",
       "      <td>52.62</td>\n",
       "      <td>56.29</td>\n",
       "      <td>49.11</td>\n",
       "      <td>52.15</td>\n",
       "      <td>...</td>\n",
       "      <td>9420.0</td>\n",
       "      <td>9680.0</td>\n",
       "      <td>9315.0</td>\n",
       "      <td>9540.0</td>\n",
       "      <td>9520.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9650.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20081202</td>\n",
       "      <td>446.908899</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>459.323035</td>\n",
       "      <td>55.99</td>\n",
       "      <td>56.29</td>\n",
       "      <td>54.68</td>\n",
       "      <td>55.30</td>\n",
       "      <td>51.80</td>\n",
       "      <td>52.24</td>\n",
       "      <td>...</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9430.0</td>\n",
       "      <td>9510.0</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>9730.0</td>\n",
       "      <td>9560.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>20081203</td>\n",
       "      <td>453.484820</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>459.696940</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.72</td>\n",
       "      <td>54.65</td>\n",
       "      <td>55.21</td>\n",
       "      <td>53.01</td>\n",
       "      <td>53.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9580.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9530.0</td>\n",
       "      <td>9690.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>9590.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>20081204</td>\n",
       "      <td>447.532919</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>459.964389</td>\n",
       "      <td>55.50</td>\n",
       "      <td>57.81</td>\n",
       "      <td>54.88</td>\n",
       "      <td>57.62</td>\n",
       "      <td>51.75</td>\n",
       "      <td>54.42</td>\n",
       "      <td>...</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>9145.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9445.0</td>\n",
       "      <td>9205.0</td>\n",
       "      <td>9225.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>20081205</td>\n",
       "      <td>447.084228</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>459.503235</td>\n",
       "      <td>58.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>56.63</td>\n",
       "      <td>56.75</td>\n",
       "      <td>54.70</td>\n",
       "      <td>56.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8710.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>8595.0</td>\n",
       "      <td>8665.0</td>\n",
       "      <td>8885.0</td>\n",
       "      <td>8940.0</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          p0          p1          p2  f000_open  f000_high  \\\n",
       "109  20081201  444.511058  457.032497  457.032497      53.08      56.33   \n",
       "110  20081202  446.908899  465.530103  459.323035      55.99      56.29   \n",
       "111  20081203  453.484820  482.060575  459.696940      56.50      56.72   \n",
       "112  20081204  447.532919  472.395859  459.964389      55.50      57.81   \n",
       "113  20081205  447.084228  471.922241  459.503235      58.50      60.00   \n",
       "\n",
       "     f000_low  f000_settle  f001_open  f001_high    ...      f136_open  \\\n",
       "109     52.62        56.29      49.11      52.15    ...         9420.0   \n",
       "110     54.68        55.30      51.80      52.24    ...         9480.0   \n",
       "111     54.65        55.21      53.01      53.02    ...         9495.0   \n",
       "112     54.88        57.62      51.75      54.42    ...         9485.0   \n",
       "113     56.63        56.75      54.70      56.18    ...         8710.0   \n",
       "\n",
       "     f136_high  f136_low  f136_settle  f137_open  f137_high  f137_low  \\\n",
       "109     9680.0    9315.0       9540.0     9520.0     9800.0    9495.0   \n",
       "110     9600.0    9430.0       9510.0     9640.0     9730.0    9560.0   \n",
       "111     9580.0    9400.0       9500.0     9530.0     9690.0    9505.0   \n",
       "112     9485.0    9120.0       9145.0     9400.0     9445.0    9205.0   \n",
       "113     9000.0    8595.0       8665.0     8885.0     8940.0    8670.0   \n",
       "\n",
       "     f137_settle  week  week_date  \n",
       "109       9650.0  2030   20081201  \n",
       "110       9630.0  2030   20081201  \n",
       "111       9590.0  2030   20081201  \n",
       "112       9225.0  2030   20081201  \n",
       "113       8715.0  2030   20081201  \n",
       "\n",
       "[5 rows x 558 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Bis. Boosted data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_t</th>\n",
       "      <th>p1_(t-99)</th>\n",
       "      <th>p1_(t-98)</th>\n",
       "      <th>p1_(t-97)</th>\n",
       "      <th>p1_(t-96)</th>\n",
       "      <th>p1_(t-95)</th>\n",
       "      <th>p1_(t-94)</th>\n",
       "      <th>p1_(t-93)</th>\n",
       "      <th>p1_(t-92)</th>\n",
       "      <th>p1_(t-91)</th>\n",
       "      <th>...</th>\n",
       "      <th>p1_(t+41)</th>\n",
       "      <th>p1_(t+42)</th>\n",
       "      <th>p1_(t+43)</th>\n",
       "      <th>p1_(t+44)</th>\n",
       "      <th>p1_(t+45)</th>\n",
       "      <th>p1_(t+46)</th>\n",
       "      <th>p1_(t+47)</th>\n",
       "      <th>p1_(t+48)</th>\n",
       "      <th>p1_(t+49)</th>\n",
       "      <th>p1_(t+50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>20090515.0</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>...</td>\n",
       "      <td>512.965645</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>20090519.0</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>...</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20090520.0</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>...</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>20090521.0</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>474.348364</td>\n",
       "      <td>...</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>20090522.0</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>472.073147</td>\n",
       "      <td>472.134908</td>\n",
       "      <td>472.519568</td>\n",
       "      <td>473.139085</td>\n",
       "      <td>467.806318</td>\n",
       "      <td>468.585995</td>\n",
       "      <td>474.348364</td>\n",
       "      <td>493.223915</td>\n",
       "      <td>...</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "      <td>560.517007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_t   p1_(t-99)   p1_(t-98)   p1_(t-97)   p1_(t-96)   p1_(t-95)  \\\n",
       "100  20090515.0  457.032497  465.530103  482.060575  472.395859  471.922241   \n",
       "101  20090519.0  465.530103  482.060575  472.395859  471.922241  472.073147   \n",
       "102  20090520.0  482.060575  472.395859  471.922241  472.073147  472.134908   \n",
       "103  20090521.0  472.395859  471.922241  472.073147  472.134908  472.519568   \n",
       "104  20090522.0  471.922241  472.073147  472.134908  472.519568  473.139085   \n",
       "\n",
       "      p1_(t-94)   p1_(t-93)   p1_(t-92)   p1_(t-91)     ...       p1_(t+41)  \\\n",
       "100  472.073147  472.134908  472.519568  473.139085     ...      512.965645   \n",
       "101  472.134908  472.519568  473.139085  467.806318     ...      516.764439   \n",
       "102  472.519568  473.139085  467.806318  468.585995     ...      516.749309   \n",
       "103  473.139085  467.806318  468.585995  474.348364     ...      516.772004   \n",
       "104  467.806318  468.585995  474.348364  493.223915     ...      519.259329   \n",
       "\n",
       "      p1_(t+42)   p1_(t+43)   p1_(t+44)   p1_(t+45)   p1_(t+46)   p1_(t+47)  \\\n",
       "100  516.764439  516.749309  516.772004  519.259329  519.221323  519.236525   \n",
       "101  516.749309  516.772004  519.259329  519.221323  519.236525  525.538546   \n",
       "102  516.772004  519.259329  519.221323  519.236525  525.538546  525.469310   \n",
       "103  519.259329  519.221323  519.236525  525.538546  525.469310  527.917451   \n",
       "104  519.221323  519.236525  525.538546  525.469310  527.917451  556.722797   \n",
       "\n",
       "      p1_(t+48)   p1_(t+49)   p1_(t+50)  \n",
       "100  525.538546  525.469310  527.917451  \n",
       "101  525.469310  527.917451  556.722797  \n",
       "102  527.917451  556.722797  556.771695  \n",
       "103  556.722797  556.771695  556.804299  \n",
       "104  556.771695  556.804299  560.517007  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "selected_features = top_correlated(df, 50, 0.75)\n",
    "selected_features = selected_features.insert(0, 'date')\n",
    "\n",
    "df_ref = reformat_as_sequence(df, 100, 50)\n",
    "df_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trend features that will help prediction\n",
    "trend_memory = len(selected_features) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_bins(df, lag, block=None):\n",
    "    \"\"\"Return categories of trend (-1, 0, +1) over the last lag days at each date\"\"\"\n",
    "    # compute slopes \n",
    "    slopes = []\n",
    "    for t in range(lag, len(df)):\n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(np.arange(lag).reshape(-1, 1), df.iloc[(t-lag):t].values)\n",
    "        slopes.append(list(linreg.coef_.flatten()))\n",
    "    df_slopes = pd.DataFrame(slopes, columns=df.columns)\n",
    "    \n",
    "    # categorize slopes\n",
    "    df_copy_slopes = df_slopes.copy()\n",
    "    for col in df.columns:\n",
    "        if col not in block:\n",
    "            thre_low = np.quantile(df_copy_slopes[col].values, 0.33)\n",
    "            thre_high = np.quantile(df_copy_slopes[col].values, 0.66)\n",
    "            df_slopes[col] = 0\n",
    "            df_slopes[col][df_copy_slopes[col] >= thre_high] = 1\n",
    "            df_slopes[col][df_copy_slopes[col] <= thre_low] = -1\n",
    "            \n",
    "    df_slopes = df_slopes.drop(block, axis=1)\n",
    "    for col in block:\n",
    "        df_slopes[col] = df[col].values[lag:len(df)]\n",
    "        \n",
    "    return df_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# prepare for join\n",
    "df_trends = trend_bins(df[selected_features], 50, block=['date'])\n",
    "df_trends = df_trends.set_index('date')\n",
    "df_ref = df_ref.set_index('date_t')\n",
    "df_trends.index.astype('float64')\n",
    "\n",
    "# join\n",
    "df_boosted = df_trends.join(df_ref, how='inner')\n",
    "df_boosted = df_boosted.reset_index()\n",
    "df_boosted = df_boosted.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_boosted(df, batch_size, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"Get a batch from the dataframe. \n",
    "    \n",
    "    Each batch contains batch_size sequences. \n",
    "    Each sequences is made of input_seq_len values and the follwing output_seq_len \n",
    "    values of the time series.\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    n = df.shape[0]\n",
    "    np.random.seed(random_state)\n",
    "    rs = np.random.randint(0, n-df.shape[1], batch_size)\n",
    "    for _, r in zip(range(batch_size), rs):\n",
    "        X_batch.append(df.iloc[r, :-output_seq_len].values.reshape(-1, 1))\n",
    "        y_batch.append(df.iloc[r, -output_seq_len:].values.reshape(-1, 1))\n",
    "    X_batch = np.array(X_batch)\n",
    "    X_batch = np.array(X_batch).transpose((1, 0, 2))\n",
    "    y_batch = np.array(y_batch).transpose((1, 0, 2))\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  0.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  0.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  0.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[ -1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[  1.       ],\n",
       "         [  1.       ]],\n",
       " \n",
       "        [[559.7102754],\n",
       "         [485.689497 ]],\n",
       " \n",
       "        [[562.6184014],\n",
       "         [485.7535252]],\n",
       " \n",
       "        [[562.6904985],\n",
       "         [485.7108378]],\n",
       " \n",
       "        [[563.8518416],\n",
       "         [488.2359547]],\n",
       " \n",
       "        [[566.6813071],\n",
       "         [494.5171197]],\n",
       " \n",
       "        [[566.9634645],\n",
       "         [494.5968116]],\n",
       " \n",
       "        [[565.6633556],\n",
       "         [494.5678297]],\n",
       " \n",
       "        [[563.5751894],\n",
       "         [498.3240411]],\n",
       " \n",
       "        [[561.9785593],\n",
       "         [498.199971 ]],\n",
       " \n",
       "        [[560.7067922],\n",
       "         [498.2364558]],\n",
       " \n",
       "        [[556.7637054],\n",
       "         [498.199971 ]],\n",
       " \n",
       "        [[557.094426 ],\n",
       "         [498.170787 ]],\n",
       " \n",
       "        [[558.4934783],\n",
       "         [498.2145642]],\n",
       " \n",
       "        [[558.7715812],\n",
       "         [497.9738844]],\n",
       " \n",
       "        [[559.0589458],\n",
       "         [498.0905486]],\n",
       " \n",
       "        [[554.4836453],\n",
       "         [504.5696103]],\n",
       " \n",
       "        [[554.412423 ],\n",
       "         [504.4513766]],\n",
       " \n",
       "        [[554.5459799],\n",
       "         [504.5326563]],\n",
       " \n",
       "        [[554.4391292],\n",
       "         [504.5104865]],\n",
       " \n",
       "        [[554.6439625],\n",
       "         [504.4587646]],\n",
       " \n",
       "        [[554.670691 ],\n",
       "         [504.562219 ]],\n",
       " \n",
       "        [[555.784731 ],\n",
       "         [504.3036624]],\n",
       " \n",
       "        [[561.1027906],\n",
       "         [504.4957077]],\n",
       " \n",
       "        [[562.5017664],\n",
       "         [504.4957077]],\n",
       " \n",
       "        [[562.3662071],\n",
       "         [504.4883187]],\n",
       " \n",
       "        [[561.9433889],\n",
       "         [506.9919828]],\n",
       " \n",
       "        [[566.3994304],\n",
       "         [513.2135593]],\n",
       " \n",
       "        [[576.5105501],\n",
       "         [513.2586605]],\n",
       " \n",
       "        [[583.797082 ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[582.4266663],\n",
       "         [513.1008411]],\n",
       " \n",
       "        [[583.0343053],\n",
       "         [513.2210756]],\n",
       " \n",
       "        [[583.0436635],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[583.9375236],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[583.8719758],\n",
       "         [516.8098343]],\n",
       " \n",
       "        [[589.8935324],\n",
       "         [519.3961938]],\n",
       " \n",
       "        [[593.9900009],\n",
       "         [519.5559612]],\n",
       " \n",
       "        [[594.0758156],\n",
       "         [523.3271185]],\n",
       " \n",
       "        [[594.6007778],\n",
       "         [523.250472 ]],\n",
       " \n",
       "        [[594.4002282],\n",
       "         [523.1815093]],\n",
       " \n",
       "        [[594.638993 ],\n",
       "         [523.2734636]],\n",
       " \n",
       "        [[593.9910382],\n",
       "         [525.7848654]],\n",
       " \n",
       "        [[593.6949595],\n",
       "         [525.7309632]],\n",
       " \n",
       "        [[591.4789615],\n",
       "         [525.7540628]],\n",
       " \n",
       "        [[589.8281455],\n",
       "         [525.7155646]],\n",
       " \n",
       "        [[589.8755899],\n",
       "         [519.4798692]],\n",
       " \n",
       "        [[587.9992378],\n",
       "         [519.5331313]],\n",
       " \n",
       "        [[588.1410428],\n",
       "         [515.7170401]],\n",
       " \n",
       "        [[589.6858583],\n",
       "         [515.815246 ]],\n",
       " \n",
       "        [[591.0604174],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[593.5708853],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[594.7441103],\n",
       "         [513.3263271]],\n",
       " \n",
       "        [[594.581672 ],\n",
       "         [513.2361089]],\n",
       " \n",
       "        [[594.944892 ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[595.1075289],\n",
       "         [513.3112885]],\n",
       " \n",
       "        [[594.2856892],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[593.7708094],\n",
       "         [513.2962508]],\n",
       " \n",
       "        [[594.0948889],\n",
       "         [513.3338467]],\n",
       " \n",
       "        [[593.87562  ],\n",
       "         [513.2285921]],\n",
       " \n",
       "        [[593.7993904],\n",
       "         [513.1834963]],\n",
       " \n",
       "        [[599.6747791],\n",
       "         [513.3263271]],\n",
       " \n",
       "        [[596.7835829],\n",
       "         [513.2436259]],\n",
       " \n",
       "        [[597.4246067],\n",
       "         [512.9806629]],\n",
       " \n",
       "        [[593.0381702],\n",
       "         [512.6504629]],\n",
       " \n",
       "        [[586.6978329],\n",
       "         [518.1522241]],\n",
       " \n",
       "        [[582.6976102],\n",
       "         [514.4954396]],\n",
       " \n",
       "        [[583.1091793],\n",
       "         [514.55573  ]],\n",
       " \n",
       "        [[583.3151819],\n",
       "         [514.4879043]],\n",
       " \n",
       "        [[583.3151819],\n",
       "         [519.4950858]],\n",
       " \n",
       "        [[582.3822235],\n",
       "         [523.2887924]],\n",
       " \n",
       "        [[580.7753474],\n",
       "         [523.2734636]],\n",
       " \n",
       "        [[579.7842159],\n",
       "         [525.831076 ]],\n",
       " \n",
       "        [[580.0546159],\n",
       "         [525.8695911]],\n",
       " \n",
       "        [[574.362492 ],\n",
       "         [525.8541844]],\n",
       " \n",
       "        [[574.8708019],\n",
       "         [532.044209 ]],\n",
       " \n",
       "        [[563.3595826],\n",
       "         [532.0520019]],\n",
       " \n",
       "        [[563.7324256],\n",
       "         [532.1611264]],\n",
       " \n",
       "        [[558.4482072],\n",
       "         [532.0286239]],\n",
       " \n",
       "        [[563.7959141],\n",
       "         [538.4455103]],\n",
       " \n",
       "        [[563.9773886],\n",
       "         [542.0909185]],\n",
       " \n",
       "        [[559.6435662],\n",
       "         [542.0988591]],\n",
       " \n",
       "        [[559.6615739],\n",
       "         [542.0909185]],\n",
       " \n",
       "        [[561.3798717],\n",
       "         [542.1544492]],\n",
       " \n",
       "        [[561.1540298],\n",
       "         [550.8948598]],\n",
       " \n",
       "        [[561.3437248],\n",
       "         [550.8061047]],\n",
       " \n",
       "        [[560.103125 ],\n",
       "         [552.1630707]],\n",
       " \n",
       "        [[555.9477464],\n",
       "         [552.1388054]],\n",
       " \n",
       "        [[554.3039016],\n",
       "         [553.2692517]],\n",
       " \n",
       "        [[550.7093136],\n",
       "         [560.9193468]],\n",
       " \n",
       "        [[551.5444486],\n",
       "         [560.9686526]],\n",
       " \n",
       "        [[552.6647468],\n",
       "         [560.9604344]],\n",
       " \n",
       "        [[552.5043811],\n",
       "         [576.0114762]],\n",
       " \n",
       "        [[552.5845523],\n",
       "         [576.0199153]],\n",
       " \n",
       "        [[554.7138256],\n",
       "         [579.7850613]],\n",
       " \n",
       "        [[554.5079218],\n",
       "         [579.8955111]],\n",
       " \n",
       "        [[550.760899 ],\n",
       "         [582.4602067]],\n",
       " \n",
       "        [[548.4696591],\n",
       "         [584.6826419]],\n",
       " \n",
       "        [[548.8156296],\n",
       "         [588.5334648]],\n",
       " \n",
       "        [[548.3556186],\n",
       "         [586.011896 ]],\n",
       " \n",
       "        [[546.1744902],\n",
       "         [585.9861414]],\n",
       " \n",
       "        [[549.2687117],\n",
       "         [586.0376529]],\n",
       " \n",
       "        [[546.0596474],\n",
       "         [585.9775571]],\n",
       " \n",
       "        [[546.3601075],\n",
       "         [586.046239 ]],\n",
       " \n",
       "        [[546.4131642],\n",
       "         [585.8659832]],\n",
       " \n",
       "        [[548.4711367],\n",
       "         [585.8831456]],\n",
       " \n",
       "        [[543.4594242],\n",
       "         [585.8659832]],\n",
       " \n",
       "        [[542.8831901],\n",
       "         [585.934639 ]],\n",
       " \n",
       "        [[543.3096065],\n",
       "         [582.2639399]],\n",
       " \n",
       "        [[543.0331554],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[542.7909224],\n",
       "         [582.2468794]],\n",
       " \n",
       "        [[540.9057914],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[534.3967829],\n",
       "         [582.3236592]],\n",
       " \n",
       "        [[529.7991144],\n",
       "         [582.2127616]],\n",
       " \n",
       "        [[521.7699103],\n",
       "         [582.1957041]],\n",
       " \n",
       "        [[521.5067289],\n",
       "         [582.2639399]],\n",
       " \n",
       "        [[518.7253597],\n",
       "         [582.1957041]],\n",
       " \n",
       "        [[523.4936574],\n",
       "         [575.7246935]],\n",
       " \n",
       "        [[520.9891774],\n",
       "         [575.7837136]],\n",
       " \n",
       "        [[518.5834809],\n",
       "         [575.8258783]],\n",
       " \n",
       "        [[518.3383077],\n",
       "         [563.3904386]],\n",
       " \n",
       "        [[511.0527729],\n",
       "         [563.3244253]],\n",
       " \n",
       "        [[511.0193789],\n",
       "         [557.0897432]],\n",
       " \n",
       "        [[513.3757398],\n",
       "         [557.0815835]],\n",
       " \n",
       "        [[515.8122237],\n",
       "         [557.0815835]],\n",
       " \n",
       "        [[512.7062685],\n",
       "         [563.2336823]],\n",
       " \n",
       "        [[512.9571155],\n",
       "         [560.5252121]],\n",
       " \n",
       "        [[517.400413 ],\n",
       "         [560.7057884]],\n",
       " \n",
       "        [[522.9788541],\n",
       "         [560.6565288]],\n",
       " \n",
       "        [[511.188054 ],\n",
       "         [560.8289753]],\n",
       " \n",
       "        [[510.0195856],\n",
       "         [557.0734242]],\n",
       " \n",
       "        [[509.9447588],\n",
       "         [550.6367426]],\n",
       " \n",
       "        [[514.4265273],\n",
       "         [550.7496391]],\n",
       " \n",
       "        [[516.3090877],\n",
       "         [550.7173782]],\n",
       " \n",
       "        [[514.6195894],\n",
       "         [547.9252585]],\n",
       " \n",
       "        [[516.1489762],\n",
       "         [548.1579266]],\n",
       " \n",
       "        [[516.0394834],\n",
       "         [548.2301741]],\n",
       " \n",
       "        [[515.6019765],\n",
       "         [547.9653596]],\n",
       " \n",
       "        [[513.2294273],\n",
       "         [547.8450739]],\n",
       " \n",
       "        [[510.4310834],\n",
       "         [548.1017472]],\n",
       " \n",
       "        [[507.5090483],\n",
       "         [548.3345652]],\n",
       " \n",
       "        [[507.3439778],\n",
       "         [550.7708906]],\n",
       " \n",
       "        [[508.6924388],\n",
       "         [549.4532376]],\n",
       " \n",
       "        [[510.0611655],\n",
       "         [549.5500277]],\n",
       " \n",
       "        [[516.2416603],\n",
       "         [546.7893285]],\n",
       " \n",
       "        [[515.6692369],\n",
       "         [547.3931064]],\n",
       " \n",
       "        [[516.9617808],\n",
       "         [547.0387285]]]), array([[[517.3244426],\n",
       "         [540.6475147]],\n",
       " \n",
       "        [[522.5099371],\n",
       "         [529.3904334]],\n",
       " \n",
       "        [[525.2024746],\n",
       "         [529.7967132]],\n",
       " \n",
       "        [[529.1834212],\n",
       "         [523.1190247]],\n",
       " \n",
       "        [[530.5760092],\n",
       "         [517.052779 ]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just an example\n",
    "fetch_sample_boosted(df_boosted, 2, 100, 5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    scaler = StandardScaler()\n",
    "    values = df.values\n",
    "    values = scaler.fit_transform(values)\n",
    "    return pd.DataFrame(values), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>f091_settle</th>\n",
       "      <th>f029_settle</th>\n",
       "      <th>f090_settle</th>\n",
       "      <th>f131_settle</th>\n",
       "      <th>f132_settle</th>\n",
       "      <th>f130_settle</th>\n",
       "      <th>f133_settle</th>\n",
       "      <th>f028_settle</th>\n",
       "      <th>f135_settle</th>\n",
       "      <th>...</th>\n",
       "      <th>p1_(t+41)</th>\n",
       "      <th>p1_(t+42)</th>\n",
       "      <th>p1_(t+43)</th>\n",
       "      <th>p1_(t+44)</th>\n",
       "      <th>p1_(t+45)</th>\n",
       "      <th>p1_(t+46)</th>\n",
       "      <th>p1_(t+47)</th>\n",
       "      <th>p1_(t+48)</th>\n",
       "      <th>p1_(t+49)</th>\n",
       "      <th>p1_(t+50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>512.965645</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>516.764439</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>516.749309</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>516.772004</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>519.259329</td>\n",
       "      <td>519.221323</td>\n",
       "      <td>519.236525</td>\n",
       "      <td>525.538546</td>\n",
       "      <td>525.469310</td>\n",
       "      <td>527.917451</td>\n",
       "      <td>556.722797</td>\n",
       "      <td>556.771695</td>\n",
       "      <td>556.804299</td>\n",
       "      <td>560.517007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p1  f091_settle  f029_settle  f090_settle  f131_settle  f132_settle  \\\n",
       "0   0            1            1            1            1            1   \n",
       "1   0            1            1            1            1            1   \n",
       "2   0            1            1            1            1            1   \n",
       "3   0            1            1            1            1            1   \n",
       "4   0            1            1            1            1            1   \n",
       "\n",
       "   f130_settle  f133_settle  f028_settle  f135_settle     ...       p1_(t+41)  \\\n",
       "0            1            1            1            1     ...      512.965645   \n",
       "1            1            1            1            1     ...      516.764439   \n",
       "2            1            1            1            1     ...      516.749309   \n",
       "3            1            1            1            1     ...      516.772004   \n",
       "4            1            1            1            1     ...      519.259329   \n",
       "\n",
       "    p1_(t+42)   p1_(t+43)   p1_(t+44)   p1_(t+45)   p1_(t+46)   p1_(t+47)  \\\n",
       "0  516.764439  516.749309  516.772004  519.259329  519.221323  519.236525   \n",
       "1  516.749309  516.772004  519.259329  519.221323  519.236525  525.538546   \n",
       "2  516.772004  519.259329  519.221323  519.236525  525.538546  525.469310   \n",
       "3  519.259329  519.221323  519.236525  525.538546  525.469310  527.917451   \n",
       "4  519.221323  519.236525  525.538546  525.469310  527.917451  556.722797   \n",
       "\n",
       "    p1_(t+48)   p1_(t+49)   p1_(t+50)  \n",
       "0  525.538546  525.469310  527.917451  \n",
       "1  525.469310  527.917451  556.722797  \n",
       "2  527.917451  556.722797  556.771695  \n",
       "3  556.722797  556.771695  556.804299  \n",
       "4  556.771695  556.804299  560.517007  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boosted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127205</td>\n",
       "      <td>0.164588</td>\n",
       "      <td>0.164142</td>\n",
       "      <td>0.164110</td>\n",
       "      <td>0.188528</td>\n",
       "      <td>0.187886</td>\n",
       "      <td>0.187691</td>\n",
       "      <td>0.249809</td>\n",
       "      <td>0.248787</td>\n",
       "      <td>0.272714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.164438</td>\n",
       "      <td>0.164367</td>\n",
       "      <td>0.188783</td>\n",
       "      <td>0.188151</td>\n",
       "      <td>0.188036</td>\n",
       "      <td>0.250182</td>\n",
       "      <td>0.249123</td>\n",
       "      <td>0.273055</td>\n",
       "      <td>0.558208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164663</td>\n",
       "      <td>0.189043</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.188302</td>\n",
       "      <td>0.250537</td>\n",
       "      <td>0.249495</td>\n",
       "      <td>0.273394</td>\n",
       "      <td>0.558595</td>\n",
       "      <td>0.558692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164975</td>\n",
       "      <td>0.189342</td>\n",
       "      <td>0.188666</td>\n",
       "      <td>0.188557</td>\n",
       "      <td>0.250809</td>\n",
       "      <td>0.249850</td>\n",
       "      <td>0.273771</td>\n",
       "      <td>0.558980</td>\n",
       "      <td>0.559080</td>\n",
       "      <td>0.559015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.053817</td>\n",
       "      <td>1.184558</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.240421</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>1.223024</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189656</td>\n",
       "      <td>0.188965</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>0.251070</td>\n",
       "      <td>0.250122</td>\n",
       "      <td>0.274130</td>\n",
       "      <td>0.559405</td>\n",
       "      <td>0.559465</td>\n",
       "      <td>0.559403</td>\n",
       "      <td>0.595813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "1 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "2 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "3 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "4 -0.053817  1.184558  1.223024  1.180081  1.235957  1.225142  1.240421   \n",
       "\n",
       "        7         8         9      ...          157       158       159  \\\n",
       "0  1.225142  1.223024  1.211009    ...     0.127205  0.164588  0.164142   \n",
       "1  1.225142  1.223024  1.211009    ...     0.164900  0.164438  0.164367   \n",
       "2  1.225142  1.223024  1.211009    ...     0.164750  0.164663  0.189043   \n",
       "3  1.225142  1.223024  1.211009    ...     0.164975  0.189342  0.188666   \n",
       "4  1.225142  1.223024  1.211009    ...     0.189656  0.188965  0.188816   \n",
       "\n",
       "        160       161       162       163       164       165       166  \n",
       "0  0.164110  0.188528  0.187886  0.187691  0.249809  0.248787  0.272714  \n",
       "1  0.188783  0.188151  0.188036  0.250182  0.249123  0.273055  0.558208  \n",
       "2  0.188406  0.188302  0.250537  0.249495  0.273394  0.558595  0.558692  \n",
       "3  0.188557  0.250809  0.249850  0.273771  0.558980  0.559080  0.559015  \n",
       "4  0.251070  0.250122  0.274130  0.559405  0.559465  0.559403  0.595813  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized/scaled prices\n",
    "df_boosted.iloc[:, :trend_memory] = df_boosted.iloc[:, :trend_memory] * 500 \n",
    "df_boosted_sc, scaler = normalize(df_boosted)\n",
    "df_boosted_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "      <td>1.955000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.268979e-18</td>\n",
       "      <td>1.453796e-17</td>\n",
       "      <td>-3.634490e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.815184e-17</td>\n",
       "      <td>1.090347e-16</td>\n",
       "      <td>2.907592e-17</td>\n",
       "      <td>-1.453796e-17</td>\n",
       "      <td>-2.907592e-17</td>\n",
       "      <td>-7.995877e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.233665e-16</td>\n",
       "      <td>-5.815184e-17</td>\n",
       "      <td>-3.489110e-16</td>\n",
       "      <td>1.744555e-16</td>\n",
       "      <td>-4.652147e-16</td>\n",
       "      <td>1.163037e-16</td>\n",
       "      <td>-1.744555e-16</td>\n",
       "      <td>-5.815184e-16</td>\n",
       "      <td>2.326073e-16</td>\n",
       "      <td>4.070628e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "      <td>1.000256e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.277209e+00</td>\n",
       "      <td>-1.280382e+00</td>\n",
       "      <td>-1.226783e+00</td>\n",
       "      <td>-1.283409</td>\n",
       "      <td>-1.227138e+00</td>\n",
       "      <td>-1.242816e+00</td>\n",
       "      <td>-1.226540e+00</td>\n",
       "      <td>-1.242816e+00</td>\n",
       "      <td>-1.226783e+00</td>\n",
       "      <td>-1.252594e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.372922e+00</td>\n",
       "      <td>-2.372928e+00</td>\n",
       "      <td>-2.372920e+00</td>\n",
       "      <td>-2.372932e+00</td>\n",
       "      <td>-2.372946e+00</td>\n",
       "      <td>-2.372942e+00</td>\n",
       "      <td>-2.372886e+00</td>\n",
       "      <td>-2.372816e+00</td>\n",
       "      <td>-2.372734e+00</td>\n",
       "      <td>-2.372650e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.277209e+00</td>\n",
       "      <td>-1.280382e+00</td>\n",
       "      <td>-1.226783e+00</td>\n",
       "      <td>-1.283409</td>\n",
       "      <td>-1.227138e+00</td>\n",
       "      <td>-1.242816e+00</td>\n",
       "      <td>-1.226540e+00</td>\n",
       "      <td>-1.242816e+00</td>\n",
       "      <td>-1.226783e+00</td>\n",
       "      <td>-1.252594e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.665983e-01</td>\n",
       "      <td>-5.668220e-01</td>\n",
       "      <td>-5.670302e-01</td>\n",
       "      <td>-5.672169e-01</td>\n",
       "      <td>-5.674010e-01</td>\n",
       "      <td>-5.675867e-01</td>\n",
       "      <td>-5.678136e-01</td>\n",
       "      <td>-5.680482e-01</td>\n",
       "      <td>-5.682536e-01</td>\n",
       "      <td>-5.684598e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.381674e-02</td>\n",
       "      <td>-4.791188e-02</td>\n",
       "      <td>-1.879648e-03</td>\n",
       "      <td>-0.051664</td>\n",
       "      <td>4.409633e-03</td>\n",
       "      <td>-8.836679e-03</td>\n",
       "      <td>6.940299e-03</td>\n",
       "      <td>-8.836679e-03</td>\n",
       "      <td>-1.879648e-03</td>\n",
       "      <td>-2.079256e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208262e-01</td>\n",
       "      <td>2.215638e-01</td>\n",
       "      <td>2.212902e-01</td>\n",
       "      <td>2.223286e-01</td>\n",
       "      <td>2.241708e-01</td>\n",
       "      <td>2.251512e-01</td>\n",
       "      <td>2.260881e-01</td>\n",
       "      <td>2.258213e-01</td>\n",
       "      <td>2.254894e-01</td>\n",
       "      <td>2.251557e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.169576e+00</td>\n",
       "      <td>1.184558e+00</td>\n",
       "      <td>1.223024e+00</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957e+00</td>\n",
       "      <td>1.225142e+00</td>\n",
       "      <td>1.240421e+00</td>\n",
       "      <td>1.225142e+00</td>\n",
       "      <td>1.223024e+00</td>\n",
       "      <td>1.211009e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.184553e-01</td>\n",
       "      <td>7.180768e-01</td>\n",
       "      <td>7.177144e-01</td>\n",
       "      <td>7.174036e-01</td>\n",
       "      <td>7.170987e-01</td>\n",
       "      <td>7.167781e-01</td>\n",
       "      <td>7.167708e-01</td>\n",
       "      <td>7.170998e-01</td>\n",
       "      <td>7.173763e-01</td>\n",
       "      <td>7.172480e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.169576e+00</td>\n",
       "      <td>1.184558e+00</td>\n",
       "      <td>1.223024e+00</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>1.235957e+00</td>\n",
       "      <td>1.225142e+00</td>\n",
       "      <td>1.240421e+00</td>\n",
       "      <td>1.225142e+00</td>\n",
       "      <td>1.223024e+00</td>\n",
       "      <td>1.211009e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.684774e+00</td>\n",
       "      <td>1.684279e+00</td>\n",
       "      <td>1.683801e+00</td>\n",
       "      <td>1.683397e+00</td>\n",
       "      <td>1.683001e+00</td>\n",
       "      <td>1.682579e+00</td>\n",
       "      <td>1.681999e+00</td>\n",
       "      <td>1.681385e+00</td>\n",
       "      <td>1.680821e+00</td>\n",
       "      <td>1.680254e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2            3             4    \\\n",
       "count  1.955000e+03  1.955000e+03  1.955000e+03  1955.000000  1.955000e+03   \n",
       "mean   7.268979e-18  1.453796e-17 -3.634490e-17     0.000000  5.815184e-17   \n",
       "std    1.000256e+00  1.000256e+00  1.000256e+00     1.000256  1.000256e+00   \n",
       "min   -1.277209e+00 -1.280382e+00 -1.226783e+00    -1.283409 -1.227138e+00   \n",
       "25%   -1.277209e+00 -1.280382e+00 -1.226783e+00    -1.283409 -1.227138e+00   \n",
       "50%   -5.381674e-02 -4.791188e-02 -1.879648e-03    -0.051664  4.409633e-03   \n",
       "75%    1.169576e+00  1.184558e+00  1.223024e+00     1.180081  1.235957e+00   \n",
       "max    1.169576e+00  1.184558e+00  1.223024e+00     1.180081  1.235957e+00   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  1.955000e+03  1.955000e+03  1.955000e+03  1.955000e+03  1.955000e+03   \n",
       "mean   1.090347e-16  2.907592e-17 -1.453796e-17 -2.907592e-17 -7.995877e-17   \n",
       "std    1.000256e+00  1.000256e+00  1.000256e+00  1.000256e+00  1.000256e+00   \n",
       "min   -1.242816e+00 -1.226540e+00 -1.242816e+00 -1.226783e+00 -1.252594e+00   \n",
       "25%   -1.242816e+00 -1.226540e+00 -1.242816e+00 -1.226783e+00 -1.252594e+00   \n",
       "50%   -8.836679e-03  6.940299e-03 -8.836679e-03 -1.879648e-03 -2.079256e-02   \n",
       "75%    1.225142e+00  1.240421e+00  1.225142e+00  1.223024e+00  1.211009e+00   \n",
       "max    1.225142e+00  1.240421e+00  1.225142e+00  1.223024e+00  1.211009e+00   \n",
       "\n",
       "           ...                157           158           159           160  \\\n",
       "count      ...       1.955000e+03  1.955000e+03  1.955000e+03  1.955000e+03   \n",
       "mean       ...      -5.233665e-16 -5.815184e-17 -3.489110e-16  1.744555e-16   \n",
       "std        ...       1.000256e+00  1.000256e+00  1.000256e+00  1.000256e+00   \n",
       "min        ...      -2.372922e+00 -2.372928e+00 -2.372920e+00 -2.372932e+00   \n",
       "25%        ...      -5.665983e-01 -5.668220e-01 -5.670302e-01 -5.672169e-01   \n",
       "50%        ...       2.208262e-01  2.215638e-01  2.212902e-01  2.223286e-01   \n",
       "75%        ...       7.184553e-01  7.180768e-01  7.177144e-01  7.174036e-01   \n",
       "max        ...       1.684774e+00  1.684279e+00  1.683801e+00  1.683397e+00   \n",
       "\n",
       "                161           162           163           164           165  \\\n",
       "count  1.955000e+03  1.955000e+03  1.955000e+03  1.955000e+03  1.955000e+03   \n",
       "mean  -4.652147e-16  1.163037e-16 -1.744555e-16 -5.815184e-16  2.326073e-16   \n",
       "std    1.000256e+00  1.000256e+00  1.000256e+00  1.000256e+00  1.000256e+00   \n",
       "min   -2.372946e+00 -2.372942e+00 -2.372886e+00 -2.372816e+00 -2.372734e+00   \n",
       "25%   -5.674010e-01 -5.675867e-01 -5.678136e-01 -5.680482e-01 -5.682536e-01   \n",
       "50%    2.241708e-01  2.251512e-01  2.260881e-01  2.258213e-01  2.254894e-01   \n",
       "75%    7.170987e-01  7.167781e-01  7.167708e-01  7.170998e-01  7.173763e-01   \n",
       "max    1.683001e+00  1.682579e+00  1.681999e+00  1.681385e+00  1.680821e+00   \n",
       "\n",
       "                166  \n",
       "count  1.955000e+03  \n",
       "mean   4.070628e-16  \n",
       "std    1.000256e+00  \n",
       "min   -2.372650e+00  \n",
       "25%   -5.684598e-01  \n",
       "50%    2.251557e-01  \n",
       "75%    7.172480e-01  \n",
       "max    1.680254e+00  \n",
       "\n",
       "[8 rows x 167 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boosted_sc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = np.mean(scaler.scale_)\n",
    "me = np.mean(scaler.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq with GRU cells model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1564 0.8\n",
      "test 391 0.2\n"
     ]
    }
   ],
   "source": [
    "# Dataframe we work on \n",
    "df = df_boosted_sc\n",
    "\n",
    "# Proportion of samples in the training set \n",
    "train_prop = 0.8\n",
    "\n",
    "# train test split\n",
    "cut = int(train_prop * len(df))\n",
    "df_train = df[:cut]\n",
    "df_test = df[cut:]\n",
    "\n",
    "# sanity check\n",
    "print('train', len(df_train), len(df_train)/len(df))\n",
    "print('test', len(df_test), len(df_test)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths to TF seq2seq model and recurrent cells to be used in this project\n",
    "tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "tf.nn.rnn_cell = tf.contrib.rnn \n",
    "tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell # Useful for learning long-range dependencies in sequences\n",
    "\n",
    "# Data shape parameters\n",
    "batch_size = 15 # How many time series to train on before updating model's weight parameters\n",
    "output_seq_len = 50 # How many days to predict into the future\n",
    "input_seq_len = 100 + trend_memory # How many days to train on in the past + trends\n",
    "\n",
    "# Internal neural network parameters\n",
    "input_dim = output_dim = 1 # Univariate time series (predicting future values based on stream of historical values)\n",
    "hidden_dim = 50  # Number of neurons in each recurrent unit \n",
    "num_layers = 3  # Number of stacked recurrent cells (number of recurrent layers)\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.005  # Small lr helps not to diverge during training. \n",
    "epochs =  1000  # How many times we perform a training step (how many times we show a batch)\n",
    "lr_decay = 0.9  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.2  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.01  # L2 regularization of weights - reduces overfitting\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset any existing graph, close any previous session, discard old variables, and start fresh\n",
    "tf.reset_default_graph()\n",
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "with tf.variable_scope('Seq2Seq'):\n",
    "    # Input values to encoder RNN\n",
    "    encoder_inputs = [tf.placeholder(tf.float32, shape=(None, input_dim), \n",
    "                     name=\"encoder_input_{}\".format(t)) for t in range(input_seq_len)]\n",
    "    \n",
    "    # Target values for decoder RNN\n",
    "    decoder_targets = [tf.placeholder(tf.float32, shape=(None, output_dim), \n",
    "                       name=\"decoder_target_{}\".format(t)) for t in range(output_seq_len)]\n",
    "    \n",
    "    # Feed final n encoder inputs into the decoder RNN, where n = output_seq_len\n",
    "    # \"GO\", represented by 0, starts the decoder\n",
    "    decoder_inputs = [tf.zeros_like(encoder_inputs[0], dtype=np.float32, name=\"GO\")] +\\\n",
    "                      encoder_inputs[-(output_seq_len - 1):]\n",
    "    \n",
    "    # Stack hidden recurrent layers\n",
    "    cells = list()\n",
    "    for i in range(num_layers):\n",
    "        with tf.variable_scope('RNN_' + str(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Pass encoder and decoder inputs through model, retrieving output from the decoder at each prediction step\n",
    "    decoder_outputs, decoder_state = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "    \n",
    "    # Squeeze decoder output into a single value, representing the forecast at that point in the sequence\n",
    "    W_out = tf.Variable(tf.truncated_normal([hidden_dim, output_dim], seed=random_state)) # Output weight matrix\n",
    "    b_out = tf.Variable(tf.truncated_normal([output_dim], seed=random_state)) # Output bias\n",
    "    \n",
    "    # Apply a trainable, constant linear transformation to final outputs\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_Scale_Factor\")\n",
    "    reshaped_outputs = [output_scale_factor * (tf.matmul(i, W_out) + b_out) for i in decoder_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    # Compute Mean Absolute Percentage loss for output at each time step: \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, decoder_targets):\n",
    "        #output_loss += tf.reduce_mean(tf.metrics.mean_absolute_error(_Y, _y))\n",
    "        #output_loss += tf.reduce_mean(tf.abs((_Y-_y)/_Y))\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "    # Penalize model complexity with L2 regularization\n",
    "    output_loss = output_loss / len(reshaped_outputs)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "    # Add regularization term to loss function        \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    # Search for minimum of loss function with RMSProp:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum, centered=False)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(df, batch_size, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    Trains session model, attempting to optimize internal weight parameters\n",
    "    to accurately predict the number of steps into future given by output_seq_len\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to reserve for prediction (training target)\n",
    "    \"\"\"\n",
    "    X_train, y_train = fetch_sample_boosted(df=df, \n",
    "                                    batch_size=batch_size, \n",
    "                                    input_seq_len=input_seq_len, \n",
    "                                    output_seq_len=output_seq_len,\n",
    "                                    )\n",
    "    feed_dict = {encoder_inputs[t]: X_train[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_train[t] for t in range(len(decoder_targets))})\n",
    "    train_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(df, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"\n",
    "    Tests session model on a batch of random time series drawn from one of the metrics DataFrames.\n",
    "    All passed parameters should be same as those used during training.\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to set aside for prediction (training target)\n",
    "    @random_state: Controls reproducible output\n",
    "    \"\"\"\n",
    "    X_test, y_test = fetch_sample_boosted(df=df, \n",
    "                                  batch_size=1, \n",
    "                                  input_seq_len=input_seq_len, \n",
    "                                  output_seq_len=output_seq_len,\n",
    "                                  random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    test_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return test_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/1000 \ttrain loss: 16.701818466186523 \tdev loss: 22.583789825439453\n",
      "Step 100/1000 \ttrain loss: 6.687774658203125 \tdev loss: 5.443711280822754\n",
      "Step 200/1000 \ttrain loss: 1.8577454090118408 \tdev loss: 1.6187018156051636\n",
      "Step 300/1000 \ttrain loss: 0.9840919971466064 \tdev loss: 0.5100806355476379\n",
      "Step 400/1000 \ttrain loss: 0.8970128893852234 \tdev loss: 0.503220796585083\n",
      "Step 500/1000 \ttrain loss: 1.3134807348251343 \tdev loss: 0.6256465911865234\n",
      "Step 600/1000 \ttrain loss: 0.9727314114570618 \tdev loss: 0.34890633821487427\n",
      "Step 700/1000 \ttrain loss: 0.7543269395828247 \tdev loss: 0.5597156286239624\n",
      "Step 800/1000 \ttrain loss: 0.6306424140930176 \tdev loss: 0.7996417880058289\n",
      "Step 900/1000 \ttrain loss: 0.5948542952537537 \tdev loss: 0.5250611305236816\n",
      "Step 1000/1000 \ttrain loss: 0.7037971615791321 \tdev loss: 0.8430114984512329\n"
     ]
    }
   ],
   "source": [
    "# Reset variables and run passengers training ops\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(epochs + 1):\n",
    "    train_loss = train_batch(df=df_train, batch_size=batch_size, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "    # Taking the dev_loss on the same random samples serves as a validation run every 100 training runs\n",
    "    if t % 100 == 0:\n",
    "        dev_loss = test_batch(df=df_test, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "        print(\"Step {0}/{1} \\ttrain loss: {2} \\tdev loss: {3}\".format(t, epochs, train_loss[1], dev_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then every time you have specified a graph run:\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_ = scaler.mean_[trend_memory:]\n",
    "scaler.var_ = scaler.var_[trend_memory:]\n",
    "scaler.scale_ = scaler.scale_[trend_memory:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.16957553]]\n",
      "\n",
      " [[ 1.18455831]]\n",
      "\n",
      " [[ 1.22302403]]\n",
      "\n",
      " [[ 1.18008141]]\n",
      "\n",
      " [[ 1.23595707]]\n",
      "\n",
      " [[ 1.22514248]]\n",
      "\n",
      " [[ 1.24042077]]\n",
      "\n",
      " [[ 1.22514248]]\n",
      "\n",
      " [[ 1.22302403]]\n",
      "\n",
      " [[ 1.21100909]]\n",
      "\n",
      " [[ 1.19703719]]\n",
      "\n",
      " [[-1.24932138]]\n",
      "\n",
      " [[-1.25061651]]\n",
      "\n",
      " [[-0.02765303]]\n",
      "\n",
      " [[ 1.19788211]]\n",
      "\n",
      " [[ 1.19894048]]\n",
      "\n",
      " [[-0.00870216]]\n",
      "\n",
      " [[-1.46404704]]\n",
      "\n",
      " [[-1.47920752]]\n",
      "\n",
      " [[-1.53018774]]\n",
      "\n",
      " [[-1.53655534]]\n",
      "\n",
      " [[-1.52891094]]\n",
      "\n",
      " [[-1.53705367]]\n",
      "\n",
      " [[-1.5526807 ]]\n",
      "\n",
      " [[-1.52650624]]\n",
      "\n",
      " [[-1.46408484]]\n",
      "\n",
      " [[-1.50372009]]\n",
      "\n",
      " [[-1.45084413]]\n",
      "\n",
      " [[-1.45289877]]\n",
      "\n",
      " [[-1.45266476]]\n",
      "\n",
      " [[-1.42005247]]\n",
      "\n",
      " [[-1.35309705]]\n",
      "\n",
      " [[-1.36367274]]\n",
      "\n",
      " [[-1.3616325 ]]\n",
      "\n",
      " [[-1.42129524]]\n",
      "\n",
      " [[-1.46029076]]\n",
      "\n",
      " [[-1.45910893]]\n",
      "\n",
      " [[-1.37853061]]\n",
      "\n",
      " [[-1.36773661]]\n",
      "\n",
      " [[-1.41642761]]\n",
      "\n",
      " [[-1.3865099 ]]\n",
      "\n",
      " [[-1.35517342]]\n",
      "\n",
      " [[-1.3383937 ]]\n",
      "\n",
      " [[-1.3595079 ]]\n",
      "\n",
      " [[-1.34344851]]\n",
      "\n",
      " [[-1.32957314]]\n",
      "\n",
      " [[-1.34320882]]\n",
      "\n",
      " [[-1.34121792]]\n",
      "\n",
      " [[-1.32692171]]\n",
      "\n",
      " [[-1.28732268]]\n",
      "\n",
      " [[-1.2838803 ]]\n",
      "\n",
      " [[-1.28164791]]\n",
      "\n",
      " [[-1.29758136]]\n",
      "\n",
      " [[-1.29144079]]\n",
      "\n",
      " [[-1.22014183]]\n",
      "\n",
      " [[-1.18028435]]\n",
      "\n",
      " [[-1.20559163]]\n",
      "\n",
      " [[-1.17642279]]\n",
      "\n",
      " [[-1.12458487]]\n",
      "\n",
      " [[-1.09532952]]\n",
      "\n",
      " [[-1.04732743]]\n",
      "\n",
      " [[-1.08393506]]\n",
      "\n",
      " [[-1.07762667]]\n",
      "\n",
      " [[-1.13764428]]\n",
      "\n",
      " [[-1.1482867 ]]\n",
      "\n",
      " [[-1.15410538]]\n",
      "\n",
      " [[-1.23688267]]\n",
      "\n",
      " [[-1.20855222]]\n",
      "\n",
      " [[-1.12934951]]\n",
      "\n",
      " [[-1.17409438]]\n",
      "\n",
      " [[-1.20387742]]\n",
      "\n",
      " [[-1.19523671]]\n",
      "\n",
      " [[-1.24655309]]\n",
      "\n",
      " [[-1.27533856]]\n",
      "\n",
      " [[-1.28508475]]\n",
      "\n",
      " [[-1.36889762]]\n",
      "\n",
      " [[-1.32075661]]\n",
      "\n",
      " [[-1.34174419]]\n",
      "\n",
      " [[-1.278863  ]]\n",
      "\n",
      " [[-1.28798474]]\n",
      "\n",
      " [[-1.24152125]]\n",
      "\n",
      " [[-1.30043989]]\n",
      "\n",
      " [[-1.30284831]]\n",
      "\n",
      " [[-1.27891637]]\n",
      "\n",
      " [[-1.28374333]]\n",
      "\n",
      " [[-1.21298322]]\n",
      "\n",
      " [[-1.22240882]]\n",
      "\n",
      " [[-1.225067  ]]\n",
      "\n",
      " [[-1.23060092]]\n",
      "\n",
      " [[-1.18027136]]\n",
      "\n",
      " [[-1.1765863 ]]\n",
      "\n",
      " [[-1.15317154]]\n",
      "\n",
      " [[-1.14024309]]\n",
      "\n",
      " [[-1.16728577]]\n",
      "\n",
      " [[-1.18782446]]\n",
      "\n",
      " [[-1.10356845]]\n",
      "\n",
      " [[-1.10199212]]\n",
      "\n",
      " [[-1.10450429]]\n",
      "\n",
      " [[-1.09963338]]\n",
      "\n",
      " [[-1.05033281]]\n",
      "\n",
      " [[-1.01877528]]\n",
      "\n",
      " [[-0.94651571]]\n",
      "\n",
      " [[-0.8925892 ]]\n",
      "\n",
      " [[-0.84427873]]\n",
      "\n",
      " [[-0.7322185 ]]\n",
      "\n",
      " [[-0.67595224]]\n",
      "\n",
      " [[-0.6149289 ]]\n",
      "\n",
      " [[-0.61903318]]\n",
      "\n",
      " [[-0.65795215]]\n",
      "\n",
      " [[-0.73351358]]\n",
      "\n",
      " [[-0.85165967]]\n",
      "\n",
      " [[-0.87063639]]\n",
      "\n",
      " [[-0.85272441]]\n",
      "\n",
      " [[-0.86456225]]\n",
      "\n",
      " [[-0.65520319]]\n",
      "\n",
      " [[-0.6389667 ]]\n",
      "\n",
      " [[-0.54550834]]]\n",
      "[[array([[-0.420803]], dtype=float32), array([[-0.49315754]], dtype=float32), array([[-0.52100354]], dtype=float32), array([[-0.5376566]], dtype=float32), array([[-0.5500609]], dtype=float32), array([[-0.5601662]], dtype=float32), array([[-0.568256]], dtype=float32), array([[-0.57430553]], dtype=float32), array([[-0.5796054]], dtype=float32), array([[-0.58307844]], dtype=float32), array([[-0.5853557]], dtype=float32), array([[-0.58588314]], dtype=float32), array([[-0.5856283]], dtype=float32), array([[-0.5844692]], dtype=float32), array([[-0.58388317]], dtype=float32), array([[-0.58371043]], dtype=float32), array([[-0.58331746]], dtype=float32), array([[-0.582847]], dtype=float32), array([[-0.581381]], dtype=float32), array([[-0.5797511]], dtype=float32), array([[-0.57836676]], dtype=float32), array([[-0.57735485]], dtype=float32), array([[-0.5758892]], dtype=float32), array([[-0.5742963]], dtype=float32), array([[-0.57253873]], dtype=float32), array([[-0.5707477]], dtype=float32), array([[-0.5695645]], dtype=float32), array([[-0.56908596]], dtype=float32), array([[-0.56768936]], dtype=float32), array([[-0.56592196]], dtype=float32), array([[-0.5643015]], dtype=float32), array([[-0.56290025]], dtype=float32), array([[-0.5610251]], dtype=float32), array([[-0.5586688]], dtype=float32), array([[-0.55535686]], dtype=float32), array([[-0.5512821]], dtype=float32), array([[-0.54677707]], dtype=float32), array([[-0.54106283]], dtype=float32), array([[-0.5347787]], dtype=float32), array([[-0.52830064]], dtype=float32), array([[-0.522812]], dtype=float32), array([[-0.51922506]], dtype=float32), array([[-0.51811934]], dtype=float32), array([[-0.5199064]], dtype=float32), array([[-0.52269083]], dtype=float32), array([[-0.52480733]], dtype=float32), array([[-0.5261888]], dtype=float32), array([[-0.52364784]], dtype=float32), array([[-0.5193129]], dtype=float32), array([[-0.51354176]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = fetch_sample_boosted(df_test, 1, input_seq_len, output_seq_len, random_state=random_state)\n",
    "feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "res = sess.run([reshaped_outputs], feed_dict=feed_dict)\n",
    "print(X_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15017c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on sample 0.8976659558806404\n"
     ]
    }
   ],
   "source": [
    "random_state = 23\n",
    "\n",
    "def viz_prediction(random_state):\n",
    "    X_test, y_test = fetch_sample_boosted(df_train, 1, input_seq_len, output_seq_len, random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    X_plot = list(sc*X_test[:, 0, :].flatten() + me)[trend_memory:]\n",
    "    y_plot = list(sc*y_test[:, 0, :].flatten() + me)\n",
    "    y_pred = list(sc*res[:, 0, :].flatten() + me)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(X_plot + y_pred, label='prediction')\n",
    "    plt.plot(X_plot + y_plot, label='actual')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"days\")\n",
    "    plt.ylabel(\"price\")\n",
    "    plt.show()\n",
    "    return (y_pred, y_plot)\n",
    "    \n",
    "y_pred, y_true = viz_prediction(random_state)\n",
    "print(\"MAPE on sample\", score_mape(y_pred, y_true, as_days=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 188\n",
    "\n",
    "# Eval on a full batch:\n",
    "def eval_batch(random_state, batch_size):\n",
    "    X_test, y_test = fetch_sample_boosted(df_test, batch_size, input_seq_len, output_seq_len, random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    MAPE = 0\n",
    "    for b in range(batch_size):\n",
    "        y_true = list(sc*y_test[:, b, :].flatten() + me)\n",
    "        y_pred = list(sc*res[:, b, :].flatten() + me)\n",
    "        MAPE += score_mape(y_pred, y_true, as_days=True)\n",
    "    return MAPE / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on 20 batch 13.713518232858387\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE on 20 batch\", eval_batch(random_state, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
