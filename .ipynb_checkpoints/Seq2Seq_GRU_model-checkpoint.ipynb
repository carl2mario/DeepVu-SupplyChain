{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model for price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used : encoder decoder made of GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tensorflow 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf  \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "print('Imported tensorflow', tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folder scripts\n",
    "import feature_selection_feed\n",
    "from evaluation import score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>f000_open</th>\n",
       "      <th>f000_high</th>\n",
       "      <th>f000_low</th>\n",
       "      <th>f000_settle</th>\n",
       "      <th>f001_open</th>\n",
       "      <th>f001_high</th>\n",
       "      <th>...</th>\n",
       "      <th>f136_open</th>\n",
       "      <th>f136_high</th>\n",
       "      <th>f136_low</th>\n",
       "      <th>f136_settle</th>\n",
       "      <th>f137_open</th>\n",
       "      <th>f137_high</th>\n",
       "      <th>f137_low</th>\n",
       "      <th>f137_settle</th>\n",
       "      <th>week</th>\n",
       "      <th>week_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20081201</td>\n",
       "      <td>444.511058</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>53.08</td>\n",
       "      <td>56.33</td>\n",
       "      <td>52.62</td>\n",
       "      <td>56.29</td>\n",
       "      <td>49.11</td>\n",
       "      <td>52.15</td>\n",
       "      <td>...</td>\n",
       "      <td>9420.0</td>\n",
       "      <td>9680.0</td>\n",
       "      <td>9315.0</td>\n",
       "      <td>9540.0</td>\n",
       "      <td>9520.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9650.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20081202</td>\n",
       "      <td>446.908899</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>459.323035</td>\n",
       "      <td>55.99</td>\n",
       "      <td>56.29</td>\n",
       "      <td>54.68</td>\n",
       "      <td>55.30</td>\n",
       "      <td>51.80</td>\n",
       "      <td>52.24</td>\n",
       "      <td>...</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9430.0</td>\n",
       "      <td>9510.0</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>9730.0</td>\n",
       "      <td>9560.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>20081203</td>\n",
       "      <td>453.484820</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>459.696940</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.72</td>\n",
       "      <td>54.65</td>\n",
       "      <td>55.21</td>\n",
       "      <td>53.01</td>\n",
       "      <td>53.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9580.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9530.0</td>\n",
       "      <td>9690.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>9590.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>20081204</td>\n",
       "      <td>447.532919</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>459.964389</td>\n",
       "      <td>55.50</td>\n",
       "      <td>57.81</td>\n",
       "      <td>54.88</td>\n",
       "      <td>57.62</td>\n",
       "      <td>51.75</td>\n",
       "      <td>54.42</td>\n",
       "      <td>...</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>9145.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9445.0</td>\n",
       "      <td>9205.0</td>\n",
       "      <td>9225.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>20081205</td>\n",
       "      <td>447.084228</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>459.503235</td>\n",
       "      <td>58.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>56.63</td>\n",
       "      <td>56.75</td>\n",
       "      <td>54.70</td>\n",
       "      <td>56.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8710.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>8595.0</td>\n",
       "      <td>8665.0</td>\n",
       "      <td>8885.0</td>\n",
       "      <td>8940.0</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          p0          p1          p2  f000_open  f000_high  \\\n",
       "109  20081201  444.511058  457.032497  457.032497      53.08      56.33   \n",
       "110  20081202  446.908899  465.530103  459.323035      55.99      56.29   \n",
       "111  20081203  453.484820  482.060575  459.696940      56.50      56.72   \n",
       "112  20081204  447.532919  472.395859  459.964389      55.50      57.81   \n",
       "113  20081205  447.084228  471.922241  459.503235      58.50      60.00   \n",
       "\n",
       "     f000_low  f000_settle  f001_open  f001_high    ...      f136_open  \\\n",
       "109     52.62        56.29      49.11      52.15    ...         9420.0   \n",
       "110     54.68        55.30      51.80      52.24    ...         9480.0   \n",
       "111     54.65        55.21      53.01      53.02    ...         9495.0   \n",
       "112     54.88        57.62      51.75      54.42    ...         9485.0   \n",
       "113     56.63        56.75      54.70      56.18    ...         8710.0   \n",
       "\n",
       "     f136_high  f136_low  f136_settle  f137_open  f137_high  f137_low  \\\n",
       "109     9680.0    9315.0       9540.0     9520.0     9800.0    9495.0   \n",
       "110     9600.0    9430.0       9510.0     9640.0     9730.0    9560.0   \n",
       "111     9580.0    9400.0       9500.0     9530.0     9690.0    9505.0   \n",
       "112     9485.0    9120.0       9145.0     9400.0     9445.0    9205.0   \n",
       "113     9000.0    8595.0       8665.0     8885.0     8940.0    8670.0   \n",
       "\n",
       "     f137_settle  week  week_date  \n",
       "109       9650.0  2030   20081201  \n",
       "110       9630.0  2030   20081201  \n",
       "111       9590.0  2030   20081201  \n",
       "112       9225.0  2030   20081201  \n",
       "113       8715.0  2030   20081201  \n",
       "\n",
       "[5 rows x 558 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    scaler = StandardScaler()\n",
    "    values = df.values.reshape(-1, 1)\n",
    "    values = scaler.fit_transform(values)\n",
    "    return pd.DataFrame(values), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = df['p1']\n",
    "\n",
    "# normalized/scaled prices\n",
    "df_p1_sc, scaler = normalize(df_p1)\n",
    "\n",
    "#df_p1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb exmaple\n",
    "t = np.arange(0, 100, 0.05)\n",
    "x = np.sin(t)\n",
    "df_dumb = pd.DataFrame(x)\n",
    "#df_dumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2bis. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(x,bucket):\n",
    "    if(bucket%2==0): #number of buckets have to be strictly odd\n",
    "        return 0,0\n",
    "    else:    \n",
    "        dif=np.array(np.diff(x))     \n",
    "        output=np.zeros(len(x))\n",
    "        ratio=np.zeros(len(x))\n",
    "    \n",
    "#used to calculate the ratio of change in value from yesterday's value     \n",
    "        for i in range(1,len(x)): \n",
    "            ratio[i]=(100*dif[i-1]/x[i])\n",
    "            \n",
    "#values have the percentile boundary values of the buckets.\n",
    "#eg: if bucket=3, values = [33 percentile , 66 percentile] value\n",
    "\n",
    "        values=np.array(range(bucket-1))    \n",
    "        for i in range(len(values)):\n",
    "            values[i]=np.sort(ratio)[int(len(x)/bucket)*(i+1)-1]\n",
    "\n",
    "#buckets have the categorical value that needs to be filled.\n",
    "#eg: if bucket=3, buckets= [-1,0,1]\n",
    "\n",
    "        start=-int((bucket-1)/2)\n",
    "        buckets=np.array(range(bucket))        \n",
    "        for i in range(len(buckets)):\n",
    "            buckets[i]=start\n",
    "            start+=1\n",
    "            \n",
    "#This loop is used to assign the custom bucket values.\n",
    "#eg: if bucket =3, value -1 is assigned if the value is below 33 pecentile\n",
    "# value 0 for between 33 to 66 percentile\n",
    "#value 1 for above 66 percentile. \n",
    "\n",
    "        for i in range(len(ratio)): #used to assign values for \n",
    "            for j in range(len(values)):\n",
    "                if (j==0):\n",
    "                    if(ratio[i]<=values[j]):\n",
    "                        output[i]=buckets[j]\n",
    "                    else:\n",
    "                        pass\n",
    "                elif j==(len(values)-1):\n",
    "                    if(ratio[i]>values[j]):\n",
    "                        output[i]=buckets[j+1]\n",
    "                    else:\n",
    "                        pass\n",
    "                elif (ratio[i]>values[j-1]) and (ratio[i]<values[j]):\n",
    "                    output[i]=buckets[j]  \n",
    "                \n",
    "#returning the output value     \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq with GRU cells model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1684 0.8\n",
      "test 421 0.2\n"
     ]
    }
   ],
   "source": [
    "# Dataframe we work on \n",
    "df = df_p1_sc\n",
    "\n",
    "# Proportion of samples in the training set \n",
    "train_prop = 0.8\n",
    "\n",
    "# train test split\n",
    "cut = int(train_prop * len(df))\n",
    "df_train = df[:cut]\n",
    "df_test = df[cut:]\n",
    "\n",
    "# sanity check\n",
    "print('train', len(df_train), len(df_train)/len(df))\n",
    "print('test', len(df_test), len(df_test)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample(df, batch_size, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"Get a batch from the dataframe. \n",
    "    \n",
    "    Each batch contains batch_size sequences. \n",
    "    Each sequences is made of input_seq_len values and the follwing output_seq_len \n",
    "    values of the time series.\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    n = df.shape[0]\n",
    "    np.random.seed(random_state)\n",
    "    rs = np.random.randint(0, n-output_seq_len-input_seq_len, batch_size)\n",
    "    for _, r in zip(range(batch_size), rs):\n",
    "        X_batch.append(df[r:r+input_seq_len].values.reshape((-1, 1)))\n",
    "        y_batch.append(df[r+input_seq_len:r+input_seq_len+output_seq_len].values.reshape((-1, 1)))\n",
    "    X_batch = np.array(X_batch)\n",
    "    X_batch = np.array(X_batch).transpose((1, 0, 2))\n",
    "    y_batch = np.array(y_batch).transpose((1, 0, 2))\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.7106085 ],\n",
       "         [-0.73611811]],\n",
       " \n",
       "        [[ 0.75315911],\n",
       "         [-0.62754299]],\n",
       " \n",
       "        [[ 0.7823066 ],\n",
       "         [-0.62409136]],\n",
       " \n",
       "        [[ 0.77736819],\n",
       "         [-0.55832562]],\n",
       " \n",
       "        [[ 0.7812197 ],\n",
       "         [-0.57941318]],\n",
       " \n",
       "        [[ 0.77292749],\n",
       "         [-0.58583571]],\n",
       " \n",
       "        [[ 0.73838756],\n",
       "         [-0.54767554]],\n",
       " \n",
       "        [[ 0.75625656],\n",
       "         [-0.5504075 ]],\n",
       " \n",
       "        [[ 0.73349962],\n",
       "         [-0.42116959]],\n",
       " \n",
       "        [[ 0.71686841],\n",
       "         [-0.42609676]]]), array([[[ 0.7030696 ],\n",
       "         [-0.30701867]],\n",
       " \n",
       "        [[ 0.69816547],\n",
       "         [-0.32476914]],\n",
       " \n",
       "        [[ 0.84655808],\n",
       "         [-0.32386274]]]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sample(df_test, 2, 10, 3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths to TF seq2seq model and recurrent cells to be used in this project\n",
    "tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "tf.nn.rnn_cell = tf.contrib.rnn \n",
    "tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell # Useful for learning long-range dependencies in sequences\n",
    "\n",
    "# Data shape parameters\n",
    "batch_size = 15 # How many time series to train on before updating model's weight parameters\n",
    "output_seq_len = 50 # How many days to predict into the future\n",
    "input_seq_len = 100 # How many days to train on in the past\n",
    "\n",
    "# Internal neural network parameters\n",
    "input_dim = output_dim = 1 # Univariate time series (predicting future values based on stream of historical values)\n",
    "hidden_dim = 50  # Number of neurons in each recurrent unit \n",
    "num_layers = 2  # Number of stacked recurrent cells (number of recurrent layers)\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.005  # Small lr helps not to diverge during training. \n",
    "epochs =  1000 #1000  # How many times we perform a training step (how many times we show a batch)\n",
    "lr_decay = 0.9  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.2  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.01  # L2 regularization of weights - reduces overfitting\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Reset any existing graph, close any previous session, discard old variables, and start fresh\n",
    "tf.reset_default_graph()\n",
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "with tf.variable_scope('Seq2Seq'):\n",
    "    # Input values to encoder RNN\n",
    "    encoder_inputs = [tf.placeholder(tf.float32, shape=(None, input_dim), \n",
    "                     name=\"encoder_input_{}\".format(t)) for t in range(input_seq_len)]\n",
    "    \n",
    "    # Target values for decoder RNN\n",
    "    decoder_targets = [tf.placeholder(tf.float32, shape=(None, output_dim), \n",
    "                       name=\"decoder_target_{}\".format(t)) for t in range(output_seq_len)]\n",
    "    \n",
    "    # Feed final n encoder inputs into the decoder RNN, where n = output_seq_len\n",
    "    # \"GO\", represented by 0, starts the decoder\n",
    "    decoder_inputs = [tf.zeros_like(encoder_inputs[0], dtype=np.float32, name=\"GO\")] +\\\n",
    "                      encoder_inputs[-(output_seq_len - 1):]\n",
    "    \n",
    "    # Stack hidden recurrent layers\n",
    "    cells = list()\n",
    "    for i in range(num_layers):\n",
    "        with tf.variable_scope('RNN_' + str(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Pass encoder and decoder inputs through model, retrieving output from the decoder at each prediction step\n",
    "    decoder_outputs, decoder_state = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "    \n",
    "    # Squeeze decoder output into a single value, representing the forecast at that point in the sequence\n",
    "    W_out = tf.Variable(tf.truncated_normal([hidden_dim, output_dim], seed=random_state)) # Output weight matrix\n",
    "    b_out = tf.Variable(tf.truncated_normal([output_dim], seed=random_state)) # Output bias\n",
    "    \n",
    "    # Apply a trainable, constant linear transformation to final outputs\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_Scale_Factor\")\n",
    "    reshaped_outputs = [output_scale_factor * (tf.matmul(i, W_out) + b_out) for i in decoder_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    # Compute Mean Absolute Percentage loss for output at each time step: \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, decoder_targets):\n",
    "        #output_loss += tf.reduce_mean(tf.metrics.mean_absolute_error(_Y, _y))\n",
    "        #output_loss += tf.reduce_mean(tf.abs((_Y-_y)/_Y))\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "    # Penalize model complexity with L2 regularization\n",
    "    output_loss = output_loss / len(reshaped_outputs)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "    # Add regularization term to loss function        \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    # Search for minimum of loss function with RMSProp:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum, centered=False)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(df, batch_size, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    Trains session model, attempting to optimize internal weight parameters\n",
    "    to accurately predict the number of steps into future given by output_seq_len\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to reserve for prediction (training target)\n",
    "    \"\"\"\n",
    "    X_train, y_train = fetch_sample(df=df, \n",
    "                                    batch_size=batch_size, \n",
    "                                    input_seq_len=input_seq_len, \n",
    "                                    output_seq_len=output_seq_len)\n",
    "    feed_dict = {encoder_inputs[t]: X_train[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_train[t] for t in range(len(decoder_targets))})\n",
    "    train_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(df, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"\n",
    "    Tests session model on a batch of random time series drawn from one of the metrics DataFrames.\n",
    "    All passed parameters should be same as those used during training.\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to set aside for prediction (training target)\n",
    "    @random_state: Controls reproducible output\n",
    "    \"\"\"\n",
    "    X_test, y_test = fetch_sample(df=df, \n",
    "                                  batch_size=1, \n",
    "                                  input_seq_len=input_seq_len, \n",
    "                                  output_seq_len=output_seq_len,\n",
    "                                  random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    test_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return test_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/1000 \ttrain loss: 36.15195083618164 \tdev loss: 11.375436782836914\n",
      "Step 100/1000 \ttrain loss: 5.013060569763184 \tdev loss: 3.8032429218292236\n",
      "Step 200/1000 \ttrain loss: 2.252657413482666 \tdev loss: 0.9754360318183899\n",
      "Step 300/1000 \ttrain loss: 1.2818881273269653 \tdev loss: 0.4478195011615753\n",
      "Step 400/1000 \ttrain loss: 1.0134928226470947 \tdev loss: 0.40803709626197815\n",
      "Step 500/1000 \ttrain loss: 0.8499141931533813 \tdev loss: 0.3886679708957672\n",
      "Step 600/1000 \ttrain loss: 0.8505844473838806 \tdev loss: 0.37357670068740845\n",
      "Step 700/1000 \ttrain loss: 1.017378568649292 \tdev loss: 0.3782508969306946\n",
      "Step 800/1000 \ttrain loss: 1.2665603160858154 \tdev loss: 0.2273077815771103\n",
      "Step 900/1000 \ttrain loss: 0.6457265615463257 \tdev loss: 0.2035423070192337\n",
      "Step 1000/1000 \ttrain loss: 0.8665908575057983 \tdev loss: 0.3679247498512268\n"
     ]
    }
   ],
   "source": [
    "# Reset variables and run passengers training ops\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(epochs + 1):\n",
    "    train_loss = train_batch(df=df_train, batch_size=batch_size, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "    # Taking the dev_loss on the same random samples serves as a validation run every 100 training runs\n",
    "    if t % 100 == 0:\n",
    "        dev_loss = test_batch(df=df_test, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "        print(\"Step {0}/{1} \\ttrain loss: {2} \\tdev loss: {3}\".format(t, epochs, train_loss[1], dev_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then every time you have specified a graph run:\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5436977 ]]\n",
      "\n",
      " [[0.54065618]]\n",
      "\n",
      " [[0.52774951]]\n",
      "\n",
      " [[0.53161685]]\n",
      "\n",
      " [[0.5251332 ]]\n",
      "\n",
      " [[0.53484092]]\n",
      "\n",
      " [[0.53966999]]\n",
      "\n",
      " [[0.52654999]]\n",
      "\n",
      " [[0.53004001]]\n",
      "\n",
      " [[0.53002313]]\n",
      "\n",
      " [[0.53161149]]\n",
      "\n",
      " [[0.53251709]]\n",
      "\n",
      " [[0.53413808]]\n",
      "\n",
      " [[0.54212818]]\n",
      "\n",
      " [[0.54138793]]\n",
      "\n",
      " [[0.53315588]]\n",
      "\n",
      " [[0.53522297]]\n",
      "\n",
      " [[0.53936187]]\n",
      "\n",
      " [[0.53481239]]\n",
      "\n",
      " [[0.53494922]]\n",
      "\n",
      " [[0.53549685]]\n",
      "\n",
      " [[0.49808268]]\n",
      "\n",
      " [[0.5103024 ]]\n",
      "\n",
      " [[0.51835837]]\n",
      "\n",
      " [[0.51765631]]\n",
      "\n",
      " [[0.52189971]]\n",
      "\n",
      " [[0.55393955]]\n",
      "\n",
      " [[0.55553367]]\n",
      "\n",
      " [[0.55856031]]\n",
      "\n",
      " [[0.55556708]]\n",
      "\n",
      " [[0.56469062]]\n",
      "\n",
      " [[0.58315704]]\n",
      "\n",
      " [[0.57030063]]\n",
      "\n",
      " [[0.56963313]]\n",
      "\n",
      " [[0.54293742]]\n",
      "\n",
      " [[0.57058164]]\n",
      "\n",
      " [[0.55346591]]\n",
      "\n",
      " [[0.55263443]]\n",
      "\n",
      " [[0.55244404]]\n",
      "\n",
      " [[0.53577174]]\n",
      "\n",
      " [[0.52846564]]\n",
      "\n",
      " [[0.52564   ]]\n",
      "\n",
      " [[0.51529218]]\n",
      "\n",
      " [[0.49559805]]\n",
      "\n",
      " [[0.50470218]]\n",
      "\n",
      " [[0.51299823]]\n",
      "\n",
      " [[0.51990482]]\n",
      "\n",
      " [[0.51088273]]\n",
      "\n",
      " [[0.50599908]]\n",
      "\n",
      " [[0.49974682]]\n",
      "\n",
      " [[0.49768511]]\n",
      "\n",
      " [[0.48438032]]\n",
      "\n",
      " [[0.47521015]]\n",
      "\n",
      " [[0.4727083 ]]\n",
      "\n",
      " [[0.47288582]]\n",
      "\n",
      " [[0.43566436]]\n",
      "\n",
      " [[0.43136088]]\n",
      "\n",
      " [[0.42929655]]\n",
      "\n",
      " [[0.42363139]]\n",
      "\n",
      " [[0.41294224]]\n",
      "\n",
      " [[0.40820292]]\n",
      "\n",
      " [[0.40783855]]\n",
      "\n",
      " [[0.38633045]]\n",
      "\n",
      " [[0.38495028]]\n",
      "\n",
      " [[0.37105631]]\n",
      "\n",
      " [[0.3227964 ]]\n",
      "\n",
      " [[0.32684226]]\n",
      "\n",
      " [[0.27852958]]\n",
      "\n",
      " [[0.26273582]]\n",
      "\n",
      " [[0.26105931]]\n",
      "\n",
      " [[0.28191289]]\n",
      "\n",
      " [[0.28392285]]\n",
      "\n",
      " [[0.29696177]]\n",
      "\n",
      " [[0.30590671]]\n",
      "\n",
      " [[0.31734272]]\n",
      "\n",
      " [[0.34111506]]\n",
      "\n",
      " [[0.36197766]]\n",
      "\n",
      " [[0.34470342]]\n",
      "\n",
      " [[0.29831105]]\n",
      "\n",
      " [[0.29020081]]\n",
      "\n",
      " [[0.31966734]]\n",
      "\n",
      " [[0.31236342]]\n",
      "\n",
      " [[0.33314814]]\n",
      "\n",
      " [[0.32730531]]\n",
      "\n",
      " [[0.32571634]]\n",
      "\n",
      " [[0.33009611]]\n",
      "\n",
      " [[0.34008414]]\n",
      "\n",
      " [[0.3666558 ]]\n",
      "\n",
      " [[0.36699267]]\n",
      "\n",
      " [[0.38268127]]\n",
      "\n",
      " [[0.39062821]]\n",
      "\n",
      " [[0.37397755]]\n",
      "\n",
      " [[0.37794851]]\n",
      "\n",
      " [[0.37790413]]\n",
      "\n",
      " [[0.38495725]]\n",
      "\n",
      " [[0.38390839]]\n",
      "\n",
      " [[0.38113818]]\n",
      "\n",
      " [[0.37393402]]\n",
      "\n",
      " [[0.38010231]]\n",
      "\n",
      " [[0.38740317]]]\n",
      "[[array([[0.4817361]], dtype=float32), array([[0.4832568]], dtype=float32), array([[0.48052344]], dtype=float32), array([[0.47355554]], dtype=float32), array([[0.46414477]], dtype=float32), array([[0.4536868]], dtype=float32), array([[0.44373742]], dtype=float32), array([[0.4349386]], dtype=float32), array([[0.4274352]], dtype=float32), array([[0.42117688]], dtype=float32), array([[0.4161831]], dtype=float32), array([[0.41239634]], dtype=float32), array([[0.40938663]], dtype=float32), array([[0.40735713]], dtype=float32), array([[0.4060351]], dtype=float32), array([[0.4048437]], dtype=float32), array([[0.40462095]], dtype=float32), array([[0.404532]], dtype=float32), array([[0.4050268]], dtype=float32), array([[0.40628326]], dtype=float32), array([[0.4084434]], dtype=float32), array([[0.4109037]], dtype=float32), array([[0.41361165]], dtype=float32), array([[0.41638502]], dtype=float32), array([[0.41919273]], dtype=float32), array([[0.4222002]], dtype=float32), array([[0.42529434]], dtype=float32), array([[0.42778906]], dtype=float32), array([[0.42934984]], dtype=float32), array([[0.43095553]], dtype=float32), array([[0.43341845]], dtype=float32), array([[0.43597022]], dtype=float32), array([[0.43889317]], dtype=float32), array([[0.44164503]], dtype=float32), array([[0.44425333]], dtype=float32), array([[0.44687003]], dtype=float32), array([[0.4496038]], dtype=float32), array([[0.45268565]], dtype=float32), array([[0.4555648]], dtype=float32), array([[0.4584524]], dtype=float32), array([[0.46123666]], dtype=float32), array([[0.46352386]], dtype=float32), array([[0.4657854]], dtype=float32), array([[0.46806204]], dtype=float32), array([[0.47048065]], dtype=float32), array([[0.47287694]], dtype=float32), array([[0.4752025]], dtype=float32), array([[0.4774059]], dtype=float32), array([[0.4797478]], dtype=float32), array([[0.48223487]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = fetch_sample(df_test, 1, input_seq_len, output_seq_len, random_state=random_state)\n",
    "feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "res = sess.run([reshaped_outputs], feed_dict=feed_dict)\n",
    "print(X_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fcacd53d670d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviz_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAPE on sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_mape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "def viz_prediction(X_test, y_test, batch_sample_nb):\n",
    "    assert (batch_sample_nb < batch_size) & (batch_sample_nb >= 0)\n",
    "    X_test, y_test = fetch_sample(df_train, batch_size, input_seq_len, output_seq_len)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    X_plot = list(scaler.inverse_transform(X_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_plot = list(scaler.inverse_transform(y_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_pred = list(scaler.inverse_transform(res[:, batch_sample_nb].flatten()))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(X_plot + y_pred, label='prediction')\n",
    "    plt.plot(X_plot + y_plot, label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return (y_pred, y_plot)\n",
    "    \n",
    "y_pred, y_true = viz_prediction(X_test, y_test, 0)\n",
    "print(\"MAPE on sample\", score_mape(y_pred, y_true, as_days=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 21\n",
    "# Eval on a full batch:\n",
    "def eval_batch(random_state, batch_size):\n",
    "    X_test, y_test = fetch_sample(df_test, batch_size, input_seq_len, output_seq_len, random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    MAPE = 0\n",
    "    for b in range(batch_size):\n",
    "        X_plot = list(scaler.inverse_transform(X_test[:, b, :].flatten()))\n",
    "        y_true = list(scaler.inverse_transform(y_test[:, b, :].flatten()))\n",
    "        y_pred = list(scaler.inverse_transform(res[:, b].flatten()))\n",
    "        MAPE += score_mape(y_pred, y_true, as_days=True)\n",
    "    return MAPE / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on 15 batch 6.728913156447905\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE on 15 batch\", eval_batch(random_state, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
