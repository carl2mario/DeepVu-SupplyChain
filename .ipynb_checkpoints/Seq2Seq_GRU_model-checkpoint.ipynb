{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model for price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assumptions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used : encoder decoder made of GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tensorflow 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf  \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "print('Imported tensorflow', tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folder scripts\n",
    "import feature_selection_feed\n",
    "from evaluation import score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metals_daily_train.csv')\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>f000_open</th>\n",
       "      <th>f000_high</th>\n",
       "      <th>f000_low</th>\n",
       "      <th>f000_settle</th>\n",
       "      <th>f001_open</th>\n",
       "      <th>f001_high</th>\n",
       "      <th>...</th>\n",
       "      <th>f136_open</th>\n",
       "      <th>f136_high</th>\n",
       "      <th>f136_low</th>\n",
       "      <th>f136_settle</th>\n",
       "      <th>f137_open</th>\n",
       "      <th>f137_high</th>\n",
       "      <th>f137_low</th>\n",
       "      <th>f137_settle</th>\n",
       "      <th>week</th>\n",
       "      <th>week_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20081201</td>\n",
       "      <td>444.511058</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>457.032497</td>\n",
       "      <td>53.08</td>\n",
       "      <td>56.33</td>\n",
       "      <td>52.62</td>\n",
       "      <td>56.29</td>\n",
       "      <td>49.11</td>\n",
       "      <td>52.15</td>\n",
       "      <td>...</td>\n",
       "      <td>9420.0</td>\n",
       "      <td>9680.0</td>\n",
       "      <td>9315.0</td>\n",
       "      <td>9540.0</td>\n",
       "      <td>9520.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9650.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>20081202</td>\n",
       "      <td>446.908899</td>\n",
       "      <td>465.530103</td>\n",
       "      <td>459.323035</td>\n",
       "      <td>55.99</td>\n",
       "      <td>56.29</td>\n",
       "      <td>54.68</td>\n",
       "      <td>55.30</td>\n",
       "      <td>51.80</td>\n",
       "      <td>52.24</td>\n",
       "      <td>...</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9430.0</td>\n",
       "      <td>9510.0</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>9730.0</td>\n",
       "      <td>9560.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>20081203</td>\n",
       "      <td>453.484820</td>\n",
       "      <td>482.060575</td>\n",
       "      <td>459.696940</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.72</td>\n",
       "      <td>54.65</td>\n",
       "      <td>55.21</td>\n",
       "      <td>53.01</td>\n",
       "      <td>53.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9495.0</td>\n",
       "      <td>9580.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9530.0</td>\n",
       "      <td>9690.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>9590.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>20081204</td>\n",
       "      <td>447.532919</td>\n",
       "      <td>472.395859</td>\n",
       "      <td>459.964389</td>\n",
       "      <td>55.50</td>\n",
       "      <td>57.81</td>\n",
       "      <td>54.88</td>\n",
       "      <td>57.62</td>\n",
       "      <td>51.75</td>\n",
       "      <td>54.42</td>\n",
       "      <td>...</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>9145.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>9445.0</td>\n",
       "      <td>9205.0</td>\n",
       "      <td>9225.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>20081205</td>\n",
       "      <td>447.084228</td>\n",
       "      <td>471.922241</td>\n",
       "      <td>459.503235</td>\n",
       "      <td>58.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>56.63</td>\n",
       "      <td>56.75</td>\n",
       "      <td>54.70</td>\n",
       "      <td>56.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8710.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>8595.0</td>\n",
       "      <td>8665.0</td>\n",
       "      <td>8885.0</td>\n",
       "      <td>8940.0</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>2030</td>\n",
       "      <td>20081201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          p0          p1          p2  f000_open  f000_high  \\\n",
       "109  20081201  444.511058  457.032497  457.032497      53.08      56.33   \n",
       "110  20081202  446.908899  465.530103  459.323035      55.99      56.29   \n",
       "111  20081203  453.484820  482.060575  459.696940      56.50      56.72   \n",
       "112  20081204  447.532919  472.395859  459.964389      55.50      57.81   \n",
       "113  20081205  447.084228  471.922241  459.503235      58.50      60.00   \n",
       "\n",
       "     f000_low  f000_settle  f001_open  f001_high    ...      f136_open  \\\n",
       "109     52.62        56.29      49.11      52.15    ...         9420.0   \n",
       "110     54.68        55.30      51.80      52.24    ...         9480.0   \n",
       "111     54.65        55.21      53.01      53.02    ...         9495.0   \n",
       "112     54.88        57.62      51.75      54.42    ...         9485.0   \n",
       "113     56.63        56.75      54.70      56.18    ...         8710.0   \n",
       "\n",
       "     f136_high  f136_low  f136_settle  f137_open  f137_high  f137_low  \\\n",
       "109     9680.0    9315.0       9540.0     9520.0     9800.0    9495.0   \n",
       "110     9600.0    9430.0       9510.0     9640.0     9730.0    9560.0   \n",
       "111     9580.0    9400.0       9500.0     9530.0     9690.0    9505.0   \n",
       "112     9485.0    9120.0       9145.0     9400.0     9445.0    9205.0   \n",
       "113     9000.0    8595.0       8665.0     8885.0     8940.0    8670.0   \n",
       "\n",
       "     f137_settle  week  week_date  \n",
       "109       9650.0  2030   20081201  \n",
       "110       9630.0  2030   20081201  \n",
       "111       9590.0  2030   20081201  \n",
       "112       9225.0  2030   20081201  \n",
       "113       8715.0  2030   20081201  \n",
       "\n",
       "[5 rows x 558 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    scaler = StandardScaler()\n",
    "    values = df.values.reshape(-1, 1)\n",
    "    values = scaler.fit_transform(values)\n",
    "    return pd.DataFrame(values), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = df['p1']\n",
    "\n",
    "# normalized/scaled prices\n",
    "df_p1_sc, scaler = normalize(df_p1)\n",
    "\n",
    "#df_p1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb exmaple\n",
    "t = np.arange(0, 100, 0.05)\n",
    "x = np.sin(t)\n",
    "df_dumb = pd.DataFrame(x)\n",
    "#df_dumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq with GRU cells model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1684 0.8\n",
      "test 421 0.2\n"
     ]
    }
   ],
   "source": [
    "# Dataframe we work on \n",
    "df = df_p1_sc\n",
    "\n",
    "# Proportion of samples in the training set \n",
    "train_prop = 0.8\n",
    "\n",
    "# train test split\n",
    "cut = int(train_prop * len(df))\n",
    "df_train = df[:cut]\n",
    "df_test = df[cut:]\n",
    "\n",
    "# sanity check\n",
    "print('train', len(df_train), len(df_train)/len(df))\n",
    "print('test', len(df_test), len(df_test)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample(df, batch_size, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"Get a batch from the dataframe. \n",
    "    \n",
    "    Each batch contains batch_size sequences. \n",
    "    Each sequences is made of input_seq_len values and the follwing output_seq_len \n",
    "    values of the time series.\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    n = df.shape[0]\n",
    "    np.random.seed(random_state)\n",
    "    rs = np.random.randint(0, n-output_seq_len-input_seq_len, batch_size)\n",
    "    for _, r in zip(range(batch_size), rs):\n",
    "        X_batch.append(df[r:r+input_seq_len].values.reshape((-1, 1)))\n",
    "        y_batch.append(df[r+input_seq_len:r+input_seq_len+output_seq_len].values.reshape((-1, 1)))\n",
    "    X_batch = np.array(X_batch)\n",
    "    X_batch = np.array(X_batch).transpose((1, 0, 2))\n",
    "    y_batch = np.array(y_batch).transpose((1, 0, 2))\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.7106085 ],\n",
       "         [-0.73611811]],\n",
       " \n",
       "        [[ 0.75315911],\n",
       "         [-0.62754299]],\n",
       " \n",
       "        [[ 0.7823066 ],\n",
       "         [-0.62409136]],\n",
       " \n",
       "        [[ 0.77736819],\n",
       "         [-0.55832562]],\n",
       " \n",
       "        [[ 0.7812197 ],\n",
       "         [-0.57941318]],\n",
       " \n",
       "        [[ 0.77292749],\n",
       "         [-0.58583571]],\n",
       " \n",
       "        [[ 0.73838756],\n",
       "         [-0.54767554]],\n",
       " \n",
       "        [[ 0.75625656],\n",
       "         [-0.5504075 ]],\n",
       " \n",
       "        [[ 0.73349962],\n",
       "         [-0.42116959]],\n",
       " \n",
       "        [[ 0.71686841],\n",
       "         [-0.42609676]]]), array([[[ 0.7030696 ],\n",
       "         [-0.30701867]],\n",
       " \n",
       "        [[ 0.69816547],\n",
       "         [-0.32476914]],\n",
       " \n",
       "        [[ 0.84655808],\n",
       "         [-0.32386274]]]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sample(df_test, 2, 10, 3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths to TF seq2seq model and recurrent cells to be used in this project\n",
    "tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "tf.nn.rnn_cell = tf.contrib.rnn \n",
    "tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell # Useful for learning long-range dependencies in sequences\n",
    "\n",
    "# Data shape parameters\n",
    "batch_size = 15 # How many time series to train on before updating model's weight parameters\n",
    "output_seq_len = 50 # How many days to predict into the future\n",
    "input_seq_len = 100 # How many days to train on in the past\n",
    "\n",
    "# Internal neural network parameters\n",
    "input_dim = output_dim = 1 # Univariate time series (predicting future values based on stream of historical values)\n",
    "hidden_dim = 50  # Number of neurons in each recurrent unit \n",
    "num_layers = 2  # Number of stacked recurrent cells (number of recurrent layers)\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.005  # Small lr helps not to diverge during training. \n",
    "epochs =  1000 #1000  # How many times we perform a training step (how many times we show a batch)\n",
    "lr_decay = 0.9  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.2  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.01  # L2 regularization of weights - reduces overfitting\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Reset any existing graph, close any previous session, discard old variables, and start fresh\n",
    "tf.reset_default_graph()\n",
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "with tf.variable_scope('Seq2Seq'):\n",
    "    # Input values to encoder RNN\n",
    "    encoder_inputs = [tf.placeholder(tf.float32, shape=(None, input_dim), \n",
    "                     name=\"encoder_input_{}\".format(t)) for t in range(input_seq_len)]\n",
    "    \n",
    "    # Target values for decoder RNN\n",
    "    decoder_targets = [tf.placeholder(tf.float32, shape=(None, output_dim), \n",
    "                       name=\"decoder_target_{}\".format(t)) for t in range(output_seq_len)]\n",
    "    \n",
    "    # Feed final n encoder inputs into the decoder RNN, where n = output_seq_len\n",
    "    # \"GO\", represented by 0, starts the decoder\n",
    "    decoder_inputs = [tf.zeros_like(encoder_inputs[0], dtype=np.float32, name=\"GO\")] +\\\n",
    "                      encoder_inputs[-(output_seq_len - 1):]\n",
    "    \n",
    "    # Stack hidden recurrent layers\n",
    "    cells = list()\n",
    "    for i in range(num_layers):\n",
    "        with tf.variable_scope('RNN_' + str(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Pass encoder and decoder inputs through model, retrieving output from the decoder at each prediction step\n",
    "    decoder_outputs, decoder_state = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "    \n",
    "    # Squeeze decoder output into a single value, representing the forecast at that point in the sequence\n",
    "    W_out = tf.Variable(tf.truncated_normal([hidden_dim, output_dim], seed=random_state)) # Output weight matrix\n",
    "    b_out = tf.Variable(tf.truncated_normal([output_dim], seed=random_state)) # Output bias\n",
    "    \n",
    "    # Apply a trainable, constant linear transformation to final outputs\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_Scale_Factor\")\n",
    "    reshaped_outputs = [output_scale_factor * (tf.matmul(i, W_out) + b_out) for i in decoder_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    # Compute Mean Absolute Percentage loss for output at each time step: \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, decoder_targets):\n",
    "        #output_loss += tf.reduce_mean(tf.metrics.mean_absolute_error(_Y, _y))\n",
    "        #output_loss += tf.reduce_mean(tf.abs((_Y-_y)/_Y))\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "    # Penalize model complexity with L2 regularization\n",
    "    output_loss = output_loss / len(reshaped_outputs)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "    # Add regularization term to loss function        \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    # Search for minimum of loss function with RMSProp:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum, centered=False)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(df, batch_size, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    Trains session model, attempting to optimize internal weight parameters\n",
    "    to accurately predict the number of steps into future given by output_seq_len\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to reserve for prediction (training target)\n",
    "    \"\"\"\n",
    "    X_train, y_train = fetch_sample(df=df, \n",
    "                                    batch_size=batch_size, \n",
    "                                    input_seq_len=input_seq_len, \n",
    "                                    output_seq_len=output_seq_len)\n",
    "    feed_dict = {encoder_inputs[t]: X_train[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_train[t] for t in range(len(decoder_targets))})\n",
    "    train_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(df, input_seq_len, output_seq_len, random_state=None):\n",
    "    \"\"\"\n",
    "    Tests session model on a batch of random time series drawn from one of the metrics DataFrames.\n",
    "    All passed parameters should be same as those used during training.\n",
    "    \n",
    "    @df: DataFrame to sample random time series from\n",
    "    @batch_size: How many time series to sample at a time\n",
    "    @input_seq_len: How many months before for prediction (training)\n",
    "    @output_seq_len: How many months to set aside for prediction (training target)\n",
    "    @random_state: Controls reproducible output\n",
    "    \"\"\"\n",
    "    X_test, y_test = fetch_sample(df=df, \n",
    "                                  batch_size=1, \n",
    "                                  input_seq_len=input_seq_len, \n",
    "                                  output_seq_len=output_seq_len,\n",
    "                                  random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    test_loss = sess.run([train_op, loss], feed_dict)\n",
    "    return test_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/1000 \ttrain loss: 36.15195083618164 \tdev loss: 11.375436782836914\n",
      "Step 100/1000 \ttrain loss: 5.013060569763184 \tdev loss: 3.8032429218292236\n",
      "Step 200/1000 \ttrain loss: 2.252657413482666 \tdev loss: 0.9754360318183899\n",
      "Step 300/1000 \ttrain loss: 1.2818881273269653 \tdev loss: 0.4478195011615753\n",
      "Step 400/1000 \ttrain loss: 1.0134928226470947 \tdev loss: 0.40803709626197815\n",
      "Step 500/1000 \ttrain loss: 0.8499141931533813 \tdev loss: 0.3886679708957672\n",
      "Step 600/1000 \ttrain loss: 0.8505844473838806 \tdev loss: 0.37357670068740845\n",
      "Step 700/1000 \ttrain loss: 1.017378568649292 \tdev loss: 0.3782508969306946\n",
      "Step 800/1000 \ttrain loss: 1.2665603160858154 \tdev loss: 0.2273077815771103\n",
      "Step 900/1000 \ttrain loss: 0.6457265615463257 \tdev loss: 0.2035423070192337\n",
      "Step 1000/1000 \ttrain loss: 0.8665908575057983 \tdev loss: 0.3679247498512268\n"
     ]
    }
   ],
   "source": [
    "# Reset variables and run passengers training ops\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(epochs + 1):\n",
    "    train_loss = train_batch(df=df_train, batch_size=batch_size, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "    # Taking the dev_loss on the same random samples serves as a validation run every 100 training runs\n",
    "    if t % 100 == 0:\n",
    "        dev_loss = test_batch(df=df_test, input_seq_len=input_seq_len, output_seq_len=output_seq_len)\n",
    "        print(\"Step {0}/{1} \\ttrain loss: {2} \\tdev loss: {3}\".format(t, epochs, train_loss[1], dev_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then every time you have specified a graph run:\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5436977 ]]\n",
      "\n",
      " [[0.54065618]]\n",
      "\n",
      " [[0.52774951]]\n",
      "\n",
      " [[0.53161685]]\n",
      "\n",
      " [[0.5251332 ]]\n",
      "\n",
      " [[0.53484092]]\n",
      "\n",
      " [[0.53966999]]\n",
      "\n",
      " [[0.52654999]]\n",
      "\n",
      " [[0.53004001]]\n",
      "\n",
      " [[0.53002313]]\n",
      "\n",
      " [[0.53161149]]\n",
      "\n",
      " [[0.53251709]]\n",
      "\n",
      " [[0.53413808]]\n",
      "\n",
      " [[0.54212818]]\n",
      "\n",
      " [[0.54138793]]\n",
      "\n",
      " [[0.53315588]]\n",
      "\n",
      " [[0.53522297]]\n",
      "\n",
      " [[0.53936187]]\n",
      "\n",
      " [[0.53481239]]\n",
      "\n",
      " [[0.53494922]]\n",
      "\n",
      " [[0.53549685]]\n",
      "\n",
      " [[0.49808268]]\n",
      "\n",
      " [[0.5103024 ]]\n",
      "\n",
      " [[0.51835837]]\n",
      "\n",
      " [[0.51765631]]\n",
      "\n",
      " [[0.52189971]]\n",
      "\n",
      " [[0.55393955]]\n",
      "\n",
      " [[0.55553367]]\n",
      "\n",
      " [[0.55856031]]\n",
      "\n",
      " [[0.55556708]]\n",
      "\n",
      " [[0.56469062]]\n",
      "\n",
      " [[0.58315704]]\n",
      "\n",
      " [[0.57030063]]\n",
      "\n",
      " [[0.56963313]]\n",
      "\n",
      " [[0.54293742]]\n",
      "\n",
      " [[0.57058164]]\n",
      "\n",
      " [[0.55346591]]\n",
      "\n",
      " [[0.55263443]]\n",
      "\n",
      " [[0.55244404]]\n",
      "\n",
      " [[0.53577174]]\n",
      "\n",
      " [[0.52846564]]\n",
      "\n",
      " [[0.52564   ]]\n",
      "\n",
      " [[0.51529218]]\n",
      "\n",
      " [[0.49559805]]\n",
      "\n",
      " [[0.50470218]]\n",
      "\n",
      " [[0.51299823]]\n",
      "\n",
      " [[0.51990482]]\n",
      "\n",
      " [[0.51088273]]\n",
      "\n",
      " [[0.50599908]]\n",
      "\n",
      " [[0.49974682]]\n",
      "\n",
      " [[0.49768511]]\n",
      "\n",
      " [[0.48438032]]\n",
      "\n",
      " [[0.47521015]]\n",
      "\n",
      " [[0.4727083 ]]\n",
      "\n",
      " [[0.47288582]]\n",
      "\n",
      " [[0.43566436]]\n",
      "\n",
      " [[0.43136088]]\n",
      "\n",
      " [[0.42929655]]\n",
      "\n",
      " [[0.42363139]]\n",
      "\n",
      " [[0.41294224]]\n",
      "\n",
      " [[0.40820292]]\n",
      "\n",
      " [[0.40783855]]\n",
      "\n",
      " [[0.38633045]]\n",
      "\n",
      " [[0.38495028]]\n",
      "\n",
      " [[0.37105631]]\n",
      "\n",
      " [[0.3227964 ]]\n",
      "\n",
      " [[0.32684226]]\n",
      "\n",
      " [[0.27852958]]\n",
      "\n",
      " [[0.26273582]]\n",
      "\n",
      " [[0.26105931]]\n",
      "\n",
      " [[0.28191289]]\n",
      "\n",
      " [[0.28392285]]\n",
      "\n",
      " [[0.29696177]]\n",
      "\n",
      " [[0.30590671]]\n",
      "\n",
      " [[0.31734272]]\n",
      "\n",
      " [[0.34111506]]\n",
      "\n",
      " [[0.36197766]]\n",
      "\n",
      " [[0.34470342]]\n",
      "\n",
      " [[0.29831105]]\n",
      "\n",
      " [[0.29020081]]\n",
      "\n",
      " [[0.31966734]]\n",
      "\n",
      " [[0.31236342]]\n",
      "\n",
      " [[0.33314814]]\n",
      "\n",
      " [[0.32730531]]\n",
      "\n",
      " [[0.32571634]]\n",
      "\n",
      " [[0.33009611]]\n",
      "\n",
      " [[0.34008414]]\n",
      "\n",
      " [[0.3666558 ]]\n",
      "\n",
      " [[0.36699267]]\n",
      "\n",
      " [[0.38268127]]\n",
      "\n",
      " [[0.39062821]]\n",
      "\n",
      " [[0.37397755]]\n",
      "\n",
      " [[0.37794851]]\n",
      "\n",
      " [[0.37790413]]\n",
      "\n",
      " [[0.38495725]]\n",
      "\n",
      " [[0.38390839]]\n",
      "\n",
      " [[0.38113818]]\n",
      "\n",
      " [[0.37393402]]\n",
      "\n",
      " [[0.38010231]]\n",
      "\n",
      " [[0.38740317]]]\n",
      "[[array([[0.4817361]], dtype=float32), array([[0.4832568]], dtype=float32), array([[0.48052344]], dtype=float32), array([[0.47355554]], dtype=float32), array([[0.46414477]], dtype=float32), array([[0.4536868]], dtype=float32), array([[0.44373742]], dtype=float32), array([[0.4349386]], dtype=float32), array([[0.4274352]], dtype=float32), array([[0.42117688]], dtype=float32), array([[0.4161831]], dtype=float32), array([[0.41239634]], dtype=float32), array([[0.40938663]], dtype=float32), array([[0.40735713]], dtype=float32), array([[0.4060351]], dtype=float32), array([[0.4048437]], dtype=float32), array([[0.40462095]], dtype=float32), array([[0.404532]], dtype=float32), array([[0.4050268]], dtype=float32), array([[0.40628326]], dtype=float32), array([[0.4084434]], dtype=float32), array([[0.4109037]], dtype=float32), array([[0.41361165]], dtype=float32), array([[0.41638502]], dtype=float32), array([[0.41919273]], dtype=float32), array([[0.4222002]], dtype=float32), array([[0.42529434]], dtype=float32), array([[0.42778906]], dtype=float32), array([[0.42934984]], dtype=float32), array([[0.43095553]], dtype=float32), array([[0.43341845]], dtype=float32), array([[0.43597022]], dtype=float32), array([[0.43889317]], dtype=float32), array([[0.44164503]], dtype=float32), array([[0.44425333]], dtype=float32), array([[0.44687003]], dtype=float32), array([[0.4496038]], dtype=float32), array([[0.45268565]], dtype=float32), array([[0.4555648]], dtype=float32), array([[0.4584524]], dtype=float32), array([[0.46123666]], dtype=float32), array([[0.46352386]], dtype=float32), array([[0.4657854]], dtype=float32), array([[0.46806204]], dtype=float32), array([[0.47048065]], dtype=float32), array([[0.47287694]], dtype=float32), array([[0.4752025]], dtype=float32), array([[0.4774059]], dtype=float32), array([[0.4797478]], dtype=float32), array([[0.48223487]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = fetch_sample(df_test, 1, input_seq_len, output_seq_len, random_state=random_state)\n",
    "feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "res = sess.run([reshaped_outputs], feed_dict=feed_dict)\n",
    "print(X_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6eca8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on sample 6.865820199382802\n"
     ]
    }
   ],
   "source": [
    "def viz_prediction(X_test, y_test, batch_sample_nb):\n",
    "    assert (batch_sample_nb < batch_size) & (batch_sample_nb >= 0)\n",
    "    X_test, y_test = fetch_sample(df_train, batch_size, input_seq_len, output_seq_len)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    X_plot = list(scaler.inverse_transform(X_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_plot = list(scaler.inverse_transform(y_test[:, batch_sample_nb, :].flatten()))\n",
    "    y_pred = list(scaler.inverse_transform(res[:, batch_sample_nb].flatten()))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(X_plot + y_pred, label='prediction')\n",
    "    plt.plot(X_plot + y_plot, label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return (y_pred, y_plot)\n",
    "    \n",
    "y_pred, y_true = viz_prediction(X_test, y_test, 0)\n",
    "print(\"MAPE on sample\", score_mape(y_pred, y_true, as_days=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 21\n",
    "# Eval on a full batch:\n",
    "def eval_batch(random_state, batch_size):\n",
    "    X_test, y_test = fetch_sample(df_test, batch_size, input_seq_len, output_seq_len, random_state=random_state)\n",
    "    feed_dict = {encoder_inputs[t]: X_test[t] for t in range(len(encoder_inputs))}\n",
    "    feed_dict.update({decoder_targets[t]: y_test[t] for t in range(len(decoder_targets))})\n",
    "    res = sess.run([reshaped_outputs], feed_dict=feed_dict)[0]\n",
    "    res = np.array(res)\n",
    "    # shape: (output_seq_len, batch_size, input_dim)\n",
    "    res.transpose((1,0,2))\n",
    "    MAPE = 0\n",
    "    for b in range(batch_size):\n",
    "        X_plot = list(scaler.inverse_transform(X_test[:, b, :].flatten()))\n",
    "        y_true = list(scaler.inverse_transform(y_test[:, b, :].flatten()))\n",
    "        y_pred = list(scaler.inverse_transform(res[:, b].flatten()))\n",
    "        MAPE += score_mape(y_pred, y_true, as_days=True)\n",
    "    return MAPE / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on 15 batch 6.728913156447905\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE on 15 batch\", eval_batch(random_state, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
